# Testing Strategy for Smart Maintenance SaaS

ğŸ‡§ğŸ‡· **[Clique aqui para ler em PortuguÃªs](#-estratÃ©gia-de-testes-portuguÃªs)** | ğŸ‡ºğŸ‡¸ **English Version Below**

ğŸ“– **Quick Navigation**

- [ğŸ“š Main Documentation](../README.md) | [ğŸ—ï¸ System Architecture](../docs/SYSTEM_AND_ARCHITECTURE.md) | [ğŸ“¸ System Screenshots](../docs/SYSTEM_SCREENSHOTS.md)
- [ğŸš€ Future Roadmap](../docs/FUTURE_ROADMAP.md) | [ğŸš€ Deployment Status](../docs/DEPLOYMENT_STATUS.md) | [âš¡ Performance Baseline](../docs/PERFORMANCE_BASELINE.md)
- [ğŸ“ˆ Load Testing](../docs/LOAD_TESTING_INSTRUCTIONS.md) | [ğŸ”§ API Documentation](../docs/api.md) | [ğŸ“ Logging Config](../core/logging_config.md)
- [âš™ï¸ Configuration Management](../core/config/README.md) | [ğŸ“‹ Original Architecture](../docs/original_full_system_architecture.md)

---

This directory contains tests for the Smart Maintenance SaaS application. The testing strategy is designed to ensure code quality and prevent regressions while keeping tests fast and reliable.

## ğŸ“Š Current Test Status

**Total Tests: 412** | **âœ… Passed: 409** | **âŒ Failed: 1** | **âš ï¸ Errors: 2**

### Test Results Summary
- **Success Rate**: 99.3% (409/412 tests passing)
- **Known Issues**: 1 scheduling constraint failure + 2 UI dependency errors
- **Core Functionality**: 100% operational

### Known Issues

**1. Scheduling Constraint Failure (1 test):**
- **Test**: `test_full_workflow_from_ingestion_to_scheduling`
- **Issue**: No available technician slots found during business hours
- **Impact**: Low - Core system functionality works correctly
- **Details**: Scheduling agent correctly processes events but calendar service constraints prevent scheduling

**2. UI Dependency Errors (2 tests):**
- **Tests**: `test_maintenance_logs`, `test_sensor_data`
- **Issue**: UI integration test dependencies
- **Impact**: Low - UI functionality verified through other means
- **Details**: Core UI components work as verified by `final_system_test.py`

## Test Organization

```text
tests/
â”œâ”€â”€ api/                    # API endpoint tests
â”‚   â”œâ”€â”€ test_actual_api.py  # Real API endpoint validation
â”‚   â””â”€â”€ test_api_endpoints.py # Additional API endpoint tests
â”œâ”€â”€ e2e/                    # End-to-end system tests  
â”‚   â”œâ”€â”€ final_system_test.py    # Complete system validation
â”‚   â”œâ”€â”€ test_ui_functionality.py # UI integration testing
â”‚   â””â”€â”€ test_e2e_full_system_workflow.py # Full workflow testing
â”œâ”€â”€ unit/                   # Component unit tests
â”‚   â”œâ”€â”€ agents/            # Agent system unit tests
â”‚   â”œâ”€â”€ core/              # Core module unit tests
â”‚   â”œâ”€â”€ data/              # Data layer unit tests
â”‚   â”œâ”€â”€ ml/                # ML component unit tests
â”‚   â””â”€â”€ rules/             # Business rules unit tests
â”œâ”€â”€ integration/           # Service integration tests
â”‚   â”œâ”€â”€ agents/            # Agent integration tests
â”‚   â”œâ”€â”€ core/              # Core system integration tests
â”‚   â”œâ”€â”€ workflows/         # Workflow integration tests
â”‚   â”œâ”€â”€ test_full_workflow.py    # Complete workflow testing
â”‚   â””â”€â”€ test_rbac_enforcement.py # Security and RBAC tests
â”œâ”€â”€ data/                  # Test data and generators
â”‚   â””â”€â”€ generators/        # Test data generation utilities
â”œâ”€â”€ conftest.py           # Shared test configuration and fixtures
â”œâ”€â”€ test_db_example.py    # Database testing examples
â”œâ”€â”€ test_settings.py      # Configuration testing utilities
â””â”€â”€ test_validation_changes.py # Data validation tests
```

## Test Types

### Unit Tests

- Test individual components in isolation
- Fast execution, no external dependencies
- Marker: `@pytest.mark.unit`

### Integration Tests

- Test interactions between components
- May require external services like databases
- Marker: `@pytest.mark.integration`

### API Tests

- Test HTTP endpoints and responses
- Marker: `@pytest.mark.api`

### Additional Test Markers

Based on the project's `pytest.ini` configuration, the following markers are available:

- `@pytest.mark.db`: Tests that require database access
- `@pytest.mark.slow`: Tests that are known to be slow
- `@pytest.mark.smoke`: Critical path functionality tests for CI

## Database Testing Strategy

We use multiple approaches for database testing to accommodate different testing scenarios:

### 1. Docker Container Approach (Default)

We use `testcontainers` to spin up a PostgreSQL with TimescaleDB container for integration tests:

**Pros:**
- Tests run against a real database instance
- Complete isolation from development and production databases
- Tests can freely modify data without affecting other environments
- Each test session gets a fresh database state
- No need for manual database setup

**Cons:**
- Requires Docker to be installed and running
- Slower startup time for the test suite
- Resource-intensive

### 2. Dedicated Test Database Approach

Alternatively, you can use a pre-configured test database by running tests with `--no-container`:

**Pros:**
- Faster startup time for the test suite
- Doesn't require Docker
- Good for CI/CD environments with pre-configured databases

**Cons:**
- Requires manual setup of a test database
- Potential for test interference if multiple test runs occur simultaneously

## Running Tests

Use the provided script to run tests:

```bash
# Run all tests with Docker container for database
./scripts/run_tests.sh

# Run only unit tests (no database required)
./scripts/run_tests.sh -m unit

# Run only integration tests
./scripts/run_tests.sh -m integration

# Run tests without using Docker container
./scripts/run_tests.sh --no-container

# Run tests with coverage report
./scripts/run_tests.sh --cov
```

## Test Database Configuration

The test database connection is configured via the `.env.test` file or environment variables:

- When using the Docker container approach, connection details are managed automatically
- When using the direct database approach (`--no-container`), the test database URL is determined from:
  1. The `DATABASE_TEST_URL` environment variable, if set
  2. The standard `DATABASE_URL` with the database name appended with `_test`

## Best Practices

1. **Isolation**: Each test should be independent and leave no side effects
2. **Fixtures**: Use pytest fixtures for test setup and teardown
3. **Async**: Use `@pytest.mark.asyncio` for async tests
4. **Markers**: Apply appropriate markers to categorize tests
5. **Mocking**: Use mocks for external services when appropriate
6. **Coverage**: Aim for high test coverage, especially for critical components

---

## ğŸ‡§ğŸ‡· EstratÃ©gia de Testes (PortuguÃªs)

Este diretÃ³rio contÃ©m testes para a aplicaÃ§Ã£o Smart Maintenance SaaS. A estratÃ©gia de testes Ã© projetada para garantir qualidade do cÃ³digo e prevenir regressÃµes, mantendo os testes rÃ¡pidos e confiÃ¡veis.

## ğŸ“Š Status Atual dos Testes

**Total de Testes: 412** | **âœ… Aprovados: 409** | **âŒ Falharam: 1** | **âš ï¸ Erros: 2**

### Resumo dos Resultados dos Testes

- **Taxa de Sucesso**: 99,3% (409/412 testes aprovados)
- **Problemas Conhecidos**: 1 falha de restriÃ§Ã£o de agendamento + 2 erros de dependÃªncia de UI
- **Funcionalidade Principal**: 100% operacional

### Problemas Conhecidos

**1. Falha de RestriÃ§Ã£o de Agendamento (1 teste):**
- **Teste**: `test_full_workflow_from_ingestion_to_scheduling`
- **Problema**: Nenhum slot de tÃ©cnico disponÃ­vel encontrado durante horÃ¡rio comercial
- **Impacto**: Baixo - Funcionalidade principal do sistema funciona corretamente
- **Detalhes**: Agente de agendamento processa eventos corretamente, mas restriÃ§Ãµes do serviÃ§o de calendÃ¡rio impedem agendamento

**2. Erros de DependÃªncia de UI (2 testes):**
- **Testes**: `test_maintenance_logs`, `test_sensor_data`
- **Problema**: DependÃªncias de teste de integraÃ§Ã£o de UI
- **Impacto**: Baixo - Funcionalidade de UI verificada por outros meios
- **Detalhes**: Componentes principais de UI funcionam conforme verificado por `final_system_test.py`

## OrganizaÃ§Ã£o dos Testes

```text
tests/
â”œâ”€â”€ api/                    # Testes de endpoints da API
â”‚   â”œâ”€â”€ test_actual_api.py  # ValidaÃ§Ã£o de endpoints reais da API
â”‚   â””â”€â”€ test_api_endpoints.py # Testes adicionais de endpoints da API
â”œâ”€â”€ e2e/                    # Testes de sistema end-to-end
â”‚   â”œâ”€â”€ final_system_test.py    # ValidaÃ§Ã£o completa do sistema
â”‚   â”œâ”€â”€ test_ui_functionality.py # Testes de integraÃ§Ã£o de UI
â”‚   â””â”€â”€ test_e2e_full_system_workflow.py # Testes de fluxo completo
â”œâ”€â”€ unit/                   # Testes unitÃ¡rios de componentes
â”‚   â”œâ”€â”€ agents/            # Testes unitÃ¡rios do sistema de agentes
â”‚   â”œâ”€â”€ core/              # Testes unitÃ¡rios dos mÃ³dulos principais
â”‚   â”œâ”€â”€ data/              # Testes unitÃ¡rios da camada de dados
â”‚   â”œâ”€â”€ ml/                # Testes unitÃ¡rios de componentes ML
â”‚   â””â”€â”€ rules/             # Testes unitÃ¡rios de regras de negÃ³cio
â”œâ”€â”€ integration/           # Testes de integraÃ§Ã£o de serviÃ§os
â”‚   â”œâ”€â”€ agents/            # Testes de integraÃ§Ã£o de agentes
â”‚   â”œâ”€â”€ core/              # Testes de integraÃ§Ã£o do sistema principal
â”‚   â”œâ”€â”€ workflows/         # Testes de integraÃ§Ã£o de workflows
â”‚   â”œâ”€â”€ test_full_workflow.py    # Testes de workflow completo
â”‚   â””â”€â”€ test_rbac_enforcement.py # Testes de seguranÃ§a e RBAC
â”œâ”€â”€ data/                  # Dados de teste e geradores
â”‚   â””â”€â”€ generators/        # UtilitÃ¡rios de geraÃ§Ã£o de dados de teste
â”œâ”€â”€ conftest.py           # ConfiguraÃ§Ã£o e fixtures compartilhados
â”œâ”€â”€ test_db_example.py    # Exemplos de testes de banco de dados
â”œâ”€â”€ test_settings.py      # UtilitÃ¡rios de teste de configuraÃ§Ã£o
â””â”€â”€ test_validation_changes.py # Testes de validaÃ§Ã£o de dados
```

## Tipos de Teste

### Testes UnitÃ¡rios

- Testam componentes individuais isoladamente
- ExecuÃ§Ã£o rÃ¡pida, sem dependÃªncias externas
- Marcador: `@pytest.mark.unit`

### Testes de IntegraÃ§Ã£o

- Testam interaÃ§Ãµes entre componentes
- Podem requerer serviÃ§os externos como bancos de dados
- Marcador: `@pytest.mark.integration`

### Testes de API

- Testam endpoints HTTP e respostas
- Marcador: `@pytest.mark.api`

### Marcadores de Teste Adicionais

Baseado na configuraÃ§Ã£o `pytest.ini` do projeto, os seguintes marcadores estÃ£o disponÃ­veis:

- `@pytest.mark.db`: Testes que requerem acesso ao banco de dados
- `@pytest.mark.slow`: Testes que sÃ£o conhecidamente lentos
- `@pytest.mark.smoke`: Testes de funcionalidade de caminho crÃ­tico para CI

## EstratÃ©gia de Teste de Banco de Dados

Usamos mÃºltiplas abordagens para testes de banco de dados para acomodar diferentes cenÃ¡rios de teste:

### 1. Abordagem de Container Docker (PadrÃ£o)

Usamos `testcontainers` para criar um container PostgreSQL com TimescaleDB para testes de integraÃ§Ã£o:

**Vantagens:**
- Testes executam contra uma instÃ¢ncia real de banco de dados
- Isolamento completo dos bancos de desenvolvimento e produÃ§Ã£o
- Testes podem modificar dados livremente sem afetar outros ambientes
- Cada sessÃ£o de teste obtÃ©m um estado limpo de banco de dados
- NÃ£o necessita configuraÃ§Ã£o manual de banco de dados

**Desvantagens:**
- Requer Docker instalado e executando
- Tempo de inicializaÃ§Ã£o mais lento para a suÃ­te de testes
- Uso intensivo de recursos

### 2. Abordagem de Banco de Dados de Teste Dedicado

Alternativamente, vocÃª pode usar um banco de dados de teste prÃ©-configurado executando testes com `--no-container`:

**Vantagens:**
- Tempo de inicializaÃ§Ã£o mais rÃ¡pido para a suÃ­te de testes
- NÃ£o requer Docker
- Bom para ambientes CI/CD com bancos de dados prÃ©-configurados

**Desvantagens:**
- Requer configuraÃ§Ã£o manual de um banco de dados de teste
- Potencial para interferÃªncia de teste se mÃºltiplas execuÃ§Ãµes de teste ocorrerem simultaneamente

## Executando Testes

Use o script fornecido para executar testes:

```bash
# Executar todos os testes com container Docker para banco de dados
./scripts/run_tests.sh

# Executar apenas testes unitÃ¡rios (sem banco de dados necessÃ¡rio)
./scripts/run_tests.sh -m unit

# Executar apenas testes de integraÃ§Ã£o
./scripts/run_tests.sh -m integration

# Executar testes sem usar container Docker
./scripts/run_tests.sh --no-container

# Executar testes com relatÃ³rio de cobertura
./scripts/run_tests.sh --cov
```

## ConfiguraÃ§Ã£o do Banco de Dados de Teste

A conexÃ£o do banco de dados de teste Ã© configurada via arquivo `.env.test` ou variÃ¡veis de ambiente:

- Ao usar a abordagem de container Docker, detalhes de conexÃ£o sÃ£o gerenciados automaticamente
- Ao usar a abordagem de banco direto (`--no-container`), a URL do banco de teste Ã© determinada de:
  1. A variÃ¡vel de ambiente `DATABASE_TEST_URL`, se definida
  2. A `DATABASE_URL` padrÃ£o com o nome do banco acrescido de `_test`

## Melhores PrÃ¡ticas

1. **Isolamento**: Cada teste deve ser independente e nÃ£o deixar efeitos colaterais
2. **Fixtures**: Use fixtures do pytest para configuraÃ§Ã£o e limpeza de testes
3. **AssÃ­ncrono**: Use `@pytest.mark.asyncio` para testes assÃ­ncronos
4. **Marcadores**: Aplique marcadores apropriados para categorizar testes
5. **Mocking**: Use mocks para serviÃ§os externos quando apropriado
6. **Cobertura**: Busque alta cobertura de testes, especialmente para componentes crÃ­ticos

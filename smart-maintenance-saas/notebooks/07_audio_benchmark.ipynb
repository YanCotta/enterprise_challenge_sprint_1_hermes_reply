{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: The Audio Gauntlet (MIMII Sound Dataset)\n",
    "\n",
    "This notebook processes raw audio signal data from the MIMII sound dataset to build supervised anomaly detection models for industrial equipment.\n",
    "\n",
    "## Objectives:\n",
    "1. Load and process MIMII industrial machine sound data (.wav files)\n",
    "2. Extract MFCC (Mel-Frequency Cepstral Coefficients) features from audio signals\n",
    "3. Train supervised classification models to detect anomalous machine sounds\n",
    "4. Register models in MLflow for production deployment\n",
    "\n",
    "## Dataset: MIMII Sound Dataset\n",
    "- **Source**: MIMII Dataset from Hitachi, Ltd. and Tokyo Institute of Technology\n",
    "- **Type**: Industrial machine sound recordings (valve, pump, fan, slider)\n",
    "- **Structure**: Normal and abnormal sound samples organized by machine type and ID\n",
    "- **Features**: .wav files sampled at 16kHz with various noise conditions\n",
    "- **Gauntlet Focus**: 6dB SNR valve and pump sounds\n",
    "\n",
    "## Methodology:\n",
    "- **Feature Extraction**: 40 MFCC coefficients per audio sample\n",
    "- **Model**: RandomForest classifier for normal vs abnormal classification\n",
    "- **Evaluation**: Classification metrics focused on anomaly detection performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='librosa')\n",
    "\n",
    "# MLflow configuration\n",
    "tracking_uri = \"http://mlflow:5000\" if os.getenv(\"DOCKER_ENV\") == \"true\" else \"http://localhost:5000\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "mlflow.set_experiment(\"Audio Gauntlet (MIMII)\")\n",
    "\n",
    "print(f\"MLflow tracking URI set to: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs('docs/ml', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Debug: Check Dataset Accessibility ---\n",
    "\n",
    "print(\"=== Dataset Accessibility Check ===\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Contents of current directory: {os.listdir('.')}\")\n",
    "\n",
    "# Check if data directory exists\n",
    "if os.path.exists('data'):\n",
    "    print(f\"‚úÖ 'data' directory exists\")\n",
    "    print(f\"Contents of data directory: {os.listdir('data')}\")\n",
    "    \n",
    "    # Check MIMII dataset specifically\n",
    "    mimii_path = 'data/MIMII_sound_dataset'\n",
    "    if os.path.exists(mimii_path):\n",
    "        print(f\"‚úÖ MIMII dataset directory exists at: {mimii_path}\")\n",
    "        print(f\"Contents: {os.listdir(mimii_path)}\")\n",
    "        \n",
    "        # Check for the specific machine types we need\n",
    "        for machine_dir in ['6_dB_valve', '6_dB_pump']:\n",
    "            machine_path = os.path.join(mimii_path, machine_dir)\n",
    "            if os.path.exists(machine_path):\n",
    "                print(f\"‚úÖ {machine_dir} directory exists\")\n",
    "                print(f\"Contents: {os.listdir(machine_path)}\")\n",
    "                \n",
    "                # Check deeper structure\n",
    "                machine_type = machine_dir.split('_')[-1]\n",
    "                full_path = os.path.join(machine_path, machine_type)\n",
    "                if os.path.exists(full_path):\n",
    "                    print(f\"‚úÖ {full_path} exists\")\n",
    "                    id_dirs = [d for d in os.listdir(full_path) if d.startswith('id_')]\n",
    "                    print(f\"Found ID directories: {id_dirs}\")\n",
    "                    \n",
    "                    # Check for a sample audio file\n",
    "                    if id_dirs:\n",
    "                        sample_id = id_dirs[0]\n",
    "                        normal_path = os.path.join(full_path, sample_id, 'normal')\n",
    "                        if os.path.exists(normal_path):\n",
    "                            wav_files = [f for f in os.listdir(normal_path) if f.endswith('.wav')]\n",
    "                            print(f\"Sample normal audio files in {sample_id}: {len(wav_files)} files\")\n",
    "                            if wav_files:\n",
    "                                print(f\"First few files: {wav_files[:3]}\")\n",
    "                        else:\n",
    "                            print(f\"‚ùå Normal directory not found: {normal_path}\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Machine type directory not found: {full_path}\")\n",
    "            else:\n",
    "                print(f\"‚ùå {machine_dir} directory not found\")\n",
    "    else:\n",
    "        print(f\"‚ùå MIMII dataset directory not found at: {mimii_path}\")\n",
    "else:\n",
    "    print(f\"‚ùå 'data' directory not found\")\n",
    "\n",
    "print(\"=== End Debug Check ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Audio Feature Extraction ---\n",
    "\n",
    "def extract_features(file_path):\n",
    "    \"\"\"\n",
    "    Loads an audio file and extracts MFCC features.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the .wav audio file\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Array of 40 MFCC features (mean across time)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio file with librosa\n",
    "        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast', duration=10.0)\n",
    "        \n",
    "        # Extract 40 MFCC coefficients\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        \n",
    "        # Take mean across time dimension to get fixed-size feature vector\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "        \n",
    "        return mfccs_processed\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define the path to the dataset and initialize containers\n",
    "data_path = 'data/MIMII_sound_dataset/'\n",
    "machine_types = ['6_dB_valve', '6_dB_pump']\n",
    "features_list = []\n",
    "file_count = 0\n",
    "\n",
    "print(\"Starting audio feature extraction from MIMII dataset...\")\n",
    "print(f\"Target machine types: {machine_types}\")\n",
    "\n",
    "# Loop through the directory structure to process audio files\n",
    "for machine_dir in machine_types:\n",
    "    # Get machine type from directory name (e.g., 'valve')\n",
    "    machine_type = machine_dir.split('_')[-1]\n",
    "    machine_path = os.path.join(data_path, machine_dir, machine_type)\n",
    "    \n",
    "    print(f\"\\nProcessing {machine_type} sounds from: {machine_path}\")\n",
    "    \n",
    "    # Use glob to find all id directories\n",
    "    id_dirs = glob.glob(os.path.join(machine_path, 'id_*'))\n",
    "    print(f\"Found {len(id_dirs)} machine IDs for {machine_type}\")\n",
    "    \n",
    "    for id_dir in sorted(id_dirs):\n",
    "        machine_id = os.path.basename(id_dir)\n",
    "        \n",
    "        # Process normal files\n",
    "        normal_files = glob.glob(os.path.join(id_dir, 'normal', '*.wav'))\n",
    "        print(f\"Processing {len(normal_files)} normal files for {machine_id}...\")\n",
    "        \n",
    "        for file_path in tqdm(normal_files, desc=f\"Normal {machine_id}\"):\n",
    "            features = extract_features(file_path)\n",
    "            if features is not None:\n",
    "                features_list.append([*features, 0, machine_type, machine_id])\n",
    "                file_count += 1\n",
    "        \n",
    "        # Process abnormal files\n",
    "        abnormal_files = glob.glob(os.path.join(id_dir, 'abnormal', '*.wav'))\n",
    "        print(f\"Processing {len(abnormal_files)} abnormal files for {machine_id}...\")\n",
    "        \n",
    "        for file_path in tqdm(abnormal_files, desc=f\"Abnormal {machine_id}\"):\n",
    "            features = extract_features(file_path)\n",
    "            if features is not None:\n",
    "                features_list.append([*features, 1, machine_type, machine_id])\n",
    "                file_count += 1\n",
    "\n",
    "# Create DataFrame with proper column names\n",
    "feature_columns = [f'mfcc_{i}' for i in range(40)]\n",
    "df = pd.DataFrame(features_list, columns=feature_columns + ['label', 'machine_type', 'machine_id'])\n",
    "\n",
    "print(f\"\\n=== Feature Extraction Complete ===\")\n",
    "print(f\"Total audio files processed: {file_count}\")\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Class distribution:\\n{df['label'].value_counts()}\")\n",
    "print(f\"Machine type distribution:\\n{df['machine_type'].value_counts()}\")\n",
    "\n",
    "# Display sample of extracted features\n",
    "print(f\"\\nSample of extracted features:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Data Preparation and Analysis ---\n",
    "\n",
    "print(\"=== Dataset Analysis ===\")\n",
    "\n",
    "# Class balance analysis\n",
    "print(\"Class Distribution:\")\n",
    "class_counts = df['label'].value_counts()\n",
    "print(f\"Normal (0): {class_counts[0]} samples ({class_counts[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Abnormal (1): {class_counts[1]} samples ({class_counts[1]/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Machine type analysis\n",
    "print(f\"\\nMachine Type Distribution:\")\n",
    "for machine_type in df['machine_type'].unique():\n",
    "    subset = df[df['machine_type'] == machine_type]\n",
    "    normal_count = (subset['label'] == 0).sum()\n",
    "    abnormal_count = (subset['label'] == 1).sum()\n",
    "    print(f\"{machine_type}: {len(subset)} total ({normal_count} normal, {abnormal_count} abnormal)\")\n",
    "\n",
    "# One-hot encode categorical features\n",
    "print(f\"\\nPreparing features for modeling...\")\n",
    "df_encoded = pd.get_dummies(df, columns=['machine_type'], drop_first=False)\n",
    "\n",
    "# Separate features and target\n",
    "feature_cols = [col for col in df_encoded.columns if col not in ['label', 'machine_id']]\n",
    "X = df_encoded[feature_cols]\n",
    "y = df_encoded['label']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Features: {feature_cols[:5]}... (+{len(feature_cols)-5} more)\")\n",
    "\n",
    "# Split data stratified by both label and machine type to ensure representative splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train_scaled.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_scaled.shape[0]} samples\")\n",
    "print(f\"Training class distribution: {pd.Series(y_train).value_counts().to_dict()}\")\n",
    "print(f\"Test class distribution: {pd.Series(y_test).value_counts().to_dict()}\")\n",
    "\n",
    "print(\"Data preparation complete! ‚úÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Model Training and Evaluation ---\n",
    "\n",
    "with mlflow.start_run(run_name=\"RandomForest_Audio_Anomaly_MIMII\"):\n",
    "    print(\"=== üéµ Audio Gauntlet: Training RandomForest on MFCC Features ===\")\n",
    "    \n",
    "    # Log dataset and experiment metadata\n",
    "    mlflow.log_param(\"dataset\", \"MIMII_6dB_Valve_Pump\")\n",
    "    mlflow.log_param(\"feature_type\", \"MFCC_40_coefficients\")\n",
    "    mlflow.log_param(\"audio_duration_limit\", \"10_seconds\")\n",
    "    mlflow.log_param(\"total_samples\", len(df))\n",
    "    mlflow.log_param(\"train_samples\", len(X_train))\n",
    "    mlflow.log_param(\"test_samples\", len(X_test))\n",
    "    mlflow.log_param(\"feature_dimensions\", X_train_scaled.shape[1])\n",
    "    \n",
    "    # Log class distribution\n",
    "    train_normal = (y_train == 0).sum()\n",
    "    train_abnormal = (y_train == 1).sum()\n",
    "    mlflow.log_param(\"train_normal_samples\", train_normal)\n",
    "    mlflow.log_param(\"train_abnormal_samples\", train_abnormal)\n",
    "    mlflow.log_param(\"class_imbalance_ratio\", train_abnormal / train_normal)\n",
    "    \n",
    "    # Configure and train the model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced',  # Handle class imbalance\n",
    "        max_depth=10,\n",
    "        min_samples_split=5\n",
    "    )\n",
    "    \n",
    "    # Log model hyperparameters\n",
    "    mlflow.log_param(\"model_type\", \"RandomForestClassifier\")\n",
    "    mlflow.log_params(model.get_params())\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"Making predictions...\")\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]  # Probability of abnormal class\n",
    "    \n",
    "    # Generate comprehensive evaluation metrics\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Extract key metrics\n",
    "    accuracy = report['accuracy']\n",
    "    precision_normal = report['0']['precision']\n",
    "    recall_normal = report['0']['recall']\n",
    "    f1_normal = report['0']['f1-score']\n",
    "    \n",
    "    precision_abnormal = report['1']['precision']\n",
    "    recall_abnormal = report['1']['recall']\n",
    "    f1_abnormal = report['1']['f1-score']\n",
    "    \n",
    "    macro_avg_f1 = report['macro avg']['f1-score']\n",
    "    weighted_avg_f1 = report['weighted avg']['f1-score']\n",
    "    \n",
    "    # Log all metrics to MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision_normal\", precision_normal)\n",
    "    mlflow.log_metric(\"recall_normal\", recall_normal)\n",
    "    mlflow.log_metric(\"f1_score_normal\", f1_normal)\n",
    "    mlflow.log_metric(\"precision_abnormal\", precision_abnormal)\n",
    "    mlflow.log_metric(\"recall_abnormal\", recall_abnormal)\n",
    "    mlflow.log_metric(\"f1_score_abnormal\", f1_abnormal)\n",
    "    mlflow.log_metric(\"macro_avg_f1\", macro_avg_f1)\n",
    "    mlflow.log_metric(\"weighted_avg_f1\", weighted_avg_f1)\n",
    "    \n",
    "    # Log classification report as artifact\n",
    "    mlflow.log_dict(report, \"classification_report.json\")\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Log top 10 most important features\n",
    "    top_features = feature_importance.head(10)\n",
    "    for i, (_, row) in enumerate(top_features.iterrows()):\n",
    "        mlflow.log_metric(f\"feature_importance_rank_{i+1}\", row['importance'])\n",
    "        mlflow.log_param(f\"top_feature_{i+1}\", row['feature'])\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n=== üéØ Audio Anomaly Detection Results ===\")\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Abnormal Detection (Recall): {recall_abnormal:.4f}\")\n",
    "    print(f\"Abnormal Precision: {precision_abnormal:.4f}\")\n",
    "    print(f\"Abnormal F1-Score: {f1_abnormal:.4f}\")\n",
    "    print(f\"Macro Average F1: {macro_avg_f1:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüìä Top 5 Most Important Features:\")\n",
    "    for i, (_, row) in enumerate(top_features.head(5).iterrows()):\n",
    "        print(f\"  {i+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Log and register the model\n",
    "    mlflow.sklearn.log_model(\n",
    "        model, \n",
    "        \"audio_anomaly_classifier\",\n",
    "        registered_model_name=\"RandomForest_MIMII_Audio_Benchmark\"\n",
    "    )\n",
    "    \n",
    "    # Also log the scaler for complete preprocessing pipeline\n",
    "    mlflow.sklearn.log_model(\n",
    "        scaler,\n",
    "        \"feature_scaler\",\n",
    "        registered_model_name=\"MIMII_Audio_Scaler\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Models registered:\")\n",
    "    print(f\"   - RandomForest_MIMII_Audio_Benchmark\")\n",
    "    print(f\"   - MIMII_Audio_Scaler\")\n",
    "\n",
    "print(f\"\\nüéâ === Audio Gauntlet Complete! === üéâ\")\n",
    "print(f\"Successfully trained audio anomaly detection model on {len(df)} MIMII sound samples\")\n",
    "print(f\"Model achieves {f1_abnormal:.1%} F1-score for abnormal sound detection\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import os\n",
    "\n",
    "tracking_uri = \"http://mlflow:5000\" if os.getenv(\"DOCKER_ENV\") == \"true\" else \"http://localhost:5000\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "mlflow.set_experiment(\"Classification Gauntlet (AI4I)\")\n",
    "\n",
    "print(f\"MLflow tracking URI set to: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Data Loading and Preprocessing ---\n",
    "df = pd.read_csv('data/AI4I_2020_uci_dataset/ai4i2020.csv')\n",
    "df = df.drop(['UDI', 'Product ID'], axis=1)\n",
    "df = pd.get_dummies(df, columns=['Type'], drop_first=True)\n",
    "X = df.drop('Machine failure', axis=1)\n",
    "y = df['Machine failure']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data loaded and preprocessed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Baseline Model Training ---\n",
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Define our baseline models\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVC': SVC(probability=True, random_state=42),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "}\n",
    "\n",
    "baseline_results = {}\n",
    "\n",
    "print(\"ðŸš€ Starting Baseline Model Training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nðŸ“Š Training {model_name}...\")\n",
    "    \n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_baseline\") as run:\n",
    "        # Log tags for easy filtering\n",
    "        mlflow.set_tag(\"phase\", \"baseline\")\n",
    "        mlflow.set_tag(\"model_type\", model_name)\n",
    "        mlflow.set_tag(\"features\", \"standard\")\n",
    "        mlflow.set_tag(\"dataset\", \"AI4I_2020\")\n",
    "        \n",
    "        # Record training time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "        \n",
    "        # Log parameters\n",
    "        if hasattr(model, 'get_params'):\n",
    "            mlflow.log_params(model.get_params())\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        if auc_score:\n",
    "            mlflow.log_metric(\"auc_score\", auc_score)\n",
    "        mlflow.log_metric(\"training_time_seconds\", training_time)\n",
    "        \n",
    "        # Log dataset info\n",
    "        mlflow.log_metric(\"train_samples\", len(X_train))\n",
    "        mlflow.log_metric(\"test_samples\", len(X_test))\n",
    "        mlflow.log_metric(\"n_features\", X_train_scaled.shape[1])\n",
    "        \n",
    "        # Store results for summary\n",
    "        baseline_results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'auc_score': auc_score,\n",
    "            'training_time': training_time,\n",
    "            'run_id': run.info.run_id\n",
    "        }\n",
    "        \n",
    "        # Register the model\n",
    "        model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "        mlflow.sklearn.log_model(\n",
    "            model,\n",
    "            \"model\",\n",
    "            registered_model_name=f\"ai4i_classifier_{model_name.lower()}_baseline\"\n",
    "        )\n",
    "        \n",
    "        # Fix the f-string formatting issue\n",
    "        auc_display = f\"{auc_score:.4f}\" if auc_score else \"N/A\"\n",
    "        print(f\"âœ… {model_name} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}, AUC: {auc_display}\")\n",
    "        print(f\"   Training time: {training_time:.2f}s\")\n",
    "        print(f\"   Run ID: {run.info.run_id}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Baseline Training Complete!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display summary table\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(baseline_results).T\n",
    "print(\"\\nðŸ“ˆ Baseline Results Summary:\")\n",
    "print(results_df[['accuracy', 'f1_score', 'auc_score', 'training_time']].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Advanced Feature Engineering ---\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "print(\"ðŸ”§ Starting Advanced Feature Engineering...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# First, let's examine our original features\n",
    "print(f\"Original features: {list(X.columns)}\")\n",
    "print(f\"Original dataset shape: {X.shape}\")\n",
    "\n",
    "# Create a copy of our original data for feature engineering\n",
    "X_engineered = X.copy()\n",
    "\n",
    "# 1. DOMAIN-SPECIFIC FEATURES for Industrial Equipment\n",
    "print(\"\\nðŸ­ Creating Domain-Specific Features...\")\n",
    "\n",
    "# Power Efficiency Ratio (critical for industrial equipment)\n",
    "X_engineered['power_efficiency'] = X_engineered['Torque [Nm]'] / (X_engineered['Rotational speed [rpm]'] + 1e-6)\n",
    "\n",
    "# Temperature-Power Stress Index (overheating under load)\n",
    "X_engineered['temp_power_stress'] = X_engineered['Process temperature [K]'] * X_engineered['Air temperature [K]'] / (X_engineered['Tool wear [min]'] + 1)\n",
    "\n",
    "# Wear Rate (critical for predictive maintenance)\n",
    "X_engineered['wear_rate'] = X_engineered['Tool wear [min]'] / (X_engineered['Rotational speed [rpm]'] + 1e-6)\n",
    "\n",
    "# Temperature Delta (thermal stress indicator)\n",
    "X_engineered['temp_delta'] = X_engineered['Process temperature [K]'] - X_engineered['Air temperature [K]']\n",
    "\n",
    "# 2. STATISTICAL ROLLING FEATURES (temporal patterns)\n",
    "print(\"ðŸ“Š Creating Statistical Features...\")\n",
    "\n",
    "# For demonstration, we'll create rolling statistics on key numerical columns\n",
    "numerical_cols = ['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']\n",
    "\n",
    "# Sort by a pseudo-time index (using index as time proxy)\n",
    "X_sorted = X_engineered.sort_index()\n",
    "\n",
    "# Rolling means (trend indicators)\n",
    "for col in numerical_cols:\n",
    "    X_engineered[f'{col}_rolling_mean_5'] = X_sorted[col].rolling(window=5, min_periods=1).mean()\n",
    "    X_engineered[f'{col}_rolling_std_5'] = X_sorted[col].rolling(window=5, min_periods=1).std().fillna(0)\n",
    "\n",
    "# 3. INTERACTION FEATURES (non-linear relationships)\n",
    "print(\"ðŸ”— Creating Interaction Features...\")\n",
    "\n",
    "# Critical interactions for machine failure prediction\n",
    "X_engineered['torque_speed_interaction'] = X_engineered['Torque [Nm]'] * X_engineered['Rotational speed [rpm]']\n",
    "X_engineered['temp_wear_interaction'] = X_engineered['Process temperature [K]'] * X_engineered['Tool wear [min]']\n",
    "X_engineered['temp_torque_interaction'] = X_engineered['Process temperature [K]'] * X_engineered['Torque [Nm]']\n",
    "\n",
    "# 4. BINNED FEATURES (categorical insights from continuous)\n",
    "print(\"ðŸ“¦ Creating Binned Features...\")\n",
    "\n",
    "# Temperature stress levels\n",
    "X_engineered['temp_stress_level'] = pd.cut(X_engineered['temp_delta'], \n",
    "                                         bins=[-np.inf, -5, 0, 5, np.inf], \n",
    "                                         labels=['low', 'normal', 'elevated', 'high']).astype(str)\n",
    "\n",
    "# Tool wear categories\n",
    "X_engineered['wear_category'] = pd.cut(X_engineered['Tool wear [min]'], \n",
    "                                     bins=[0, 50, 150, 250, np.inf], \n",
    "                                     labels=['new', 'moderate', 'high', 'critical']).astype(str)\n",
    "\n",
    "# Convert categorical binned features to dummy variables\n",
    "X_engineered = pd.get_dummies(X_engineered, columns=['temp_stress_level', 'wear_category'], drop_first=True)\n",
    "\n",
    "# 5. POLYNOMIAL FEATURES (capture non-linear patterns)\n",
    "print(\"ðŸ§® Creating Polynomial Features...\")\n",
    "\n",
    "# Select key features for polynomial expansion (to avoid explosion)\n",
    "key_features = ['Torque [Nm]', 'Rotational speed [rpm]', 'Tool wear [min]', 'power_efficiency']\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_engineered[key_features])\n",
    "poly_feature_names = poly.get_feature_names_out(key_features)\n",
    "\n",
    "# Add polynomial features\n",
    "for i, name in enumerate(poly_feature_names):\n",
    "    if name not in key_features:  # Skip original features\n",
    "        X_engineered[f'poly_{name}'] = X_poly[:, i]\n",
    "\n",
    "print(f\"\\nâœ¨ Feature Engineering Complete!\")\n",
    "print(f\"Original features: {X.shape[1]}\")\n",
    "print(f\"Engineered features: {X_engineered.shape[1]}\")\n",
    "print(f\"New features added: {X_engineered.shape[1] - X.shape[1]}\")\n",
    "\n",
    "# Prepare engineered train/test splits\n",
    "X_train_eng, X_test_eng, _, _ = train_test_split(X_engineered, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the engineered features\n",
    "scaler_eng = StandardScaler()\n",
    "X_train_eng_scaled = scaler_eng.fit_transform(X_train_eng)\n",
    "X_test_eng_scaled = scaler_eng.transform(X_test_eng)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Engineered dataset ready for training!\")\n",
    "print(f\"Training shape: {X_train_eng_scaled.shape}\")\n",
    "print(f\"Test shape: {X_test_eng_scaled.shape}\")\n",
    "\n",
    "# Show sample of new features\n",
    "new_features = [col for col in X_engineered.columns if col not in X.columns]\n",
    "print(f\"\\nðŸ” Sample of new features created:\")\n",
    "for feature in new_features[:10]:  # Show first 10 new features\n",
    "    print(f\"  â€¢ {feature}\")\n",
    "if len(new_features) > 10:\n",
    "    print(f\"  ... and {len(new_features) - 10} more features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Feature-Engineered Model Training ---\n",
    "\n",
    "print(\"ðŸš€ Starting Feature-Engineered Model Training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define the same models for fair comparison\n",
    "models_engineered = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVC': SVC(probability=True, random_state=42),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "}\n",
    "\n",
    "engineered_results = {}\n",
    "\n",
    "for model_name, model in models_engineered.items():\n",
    "    print(f\"\\nðŸ“Š Training {model_name} (Feature-Engineered)...\")\n",
    "    \n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_engineered\") as run:\n",
    "        # Log tags for easy filtering and comparison\n",
    "        mlflow.set_tag(\"phase\", \"engineered\")\n",
    "        mlflow.set_tag(\"model_type\", model_name)\n",
    "        mlflow.set_tag(\"features\", \"engineered\")\n",
    "        mlflow.set_tag(\"dataset\", \"AI4I_2020\")\n",
    "        \n",
    "        # Record training time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train the model on engineered features\n",
    "        model.fit(X_train_eng_scaled, y_train)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_eng = model.predict(X_test_eng_scaled)\n",
    "        y_pred_proba_eng = model.predict_proba(X_test_eng_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy_eng = accuracy_score(y_test, y_pred_eng)\n",
    "        precision_eng = precision_score(y_test, y_pred_eng)\n",
    "        recall_eng = recall_score(y_test, y_pred_eng)\n",
    "        f1_eng = f1_score(y_test, y_pred_eng)\n",
    "        auc_score_eng = roc_auc_score(y_test, y_pred_proba_eng) if y_pred_proba_eng is not None else None\n",
    "        \n",
    "        # Calculate improvement over baseline\n",
    "        baseline_f1 = baseline_results[model_name]['f1_score']\n",
    "        f1_improvement = ((f1_eng - baseline_f1) / baseline_f1) * 100\n",
    "        \n",
    "        baseline_accuracy = baseline_results[model_name]['accuracy']\n",
    "        accuracy_improvement = ((accuracy_eng - baseline_accuracy) / baseline_accuracy) * 100\n",
    "        \n",
    "        # Log parameters\n",
    "        if hasattr(model, 'get_params'):\n",
    "            mlflow.log_params(model.get_params())\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy_eng)\n",
    "        mlflow.log_metric(\"precision\", precision_eng)\n",
    "        mlflow.log_metric(\"recall\", recall_eng)\n",
    "        mlflow.log_metric(\"f1_score\", f1_eng)\n",
    "        if auc_score_eng:\n",
    "            mlflow.log_metric(\"auc_score\", auc_score_eng)\n",
    "        mlflow.log_metric(\"training_time_seconds\", training_time)\n",
    "        \n",
    "        # Log improvement metrics\n",
    "        mlflow.log_metric(\"f1_improvement_percent\", f1_improvement)\n",
    "        mlflow.log_metric(\"accuracy_improvement_percent\", accuracy_improvement)\n",
    "        mlflow.log_metric(\"baseline_f1_score\", baseline_f1)\n",
    "        mlflow.log_metric(\"baseline_accuracy\", baseline_accuracy)\n",
    "        \n",
    "        # Log dataset info\n",
    "        mlflow.log_metric(\"train_samples\", len(X_train_eng))\n",
    "        mlflow.log_metric(\"test_samples\", len(X_test_eng))\n",
    "        mlflow.log_metric(\"n_features\", X_train_eng_scaled.shape[1])\n",
    "        mlflow.log_metric(\"n_engineered_features\", X_train_eng_scaled.shape[1] - X_train_scaled.shape[1])\n",
    "        \n",
    "        # Store results for summary\n",
    "        engineered_results[model_name] = {\n",
    "            'accuracy': accuracy_eng,\n",
    "            'precision': precision_eng,\n",
    "            'recall': recall_eng,\n",
    "            'f1_score': f1_eng,\n",
    "            'auc_score': auc_score_eng,\n",
    "            'training_time': training_time,\n",
    "            'f1_improvement': f1_improvement,\n",
    "            'accuracy_improvement': accuracy_improvement,\n",
    "            'run_id': run.info.run_id\n",
    "        }\n",
    "        \n",
    "        # Register the engineered model\n",
    "        model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "        mlflow.sklearn.log_model(\n",
    "            model,\n",
    "            \"model\",\n",
    "            registered_model_name=f\"ai4i_classifier_{model_name.lower()}_engineered\"\n",
    "        )\n",
    "        \n",
    "        # Fix the f-string formatting issue\n",
    "        auc_display_eng = f\"{auc_score_eng:.4f}\" if auc_score_eng else \"N/A\"\n",
    "        print(f\"âœ… {model_name} (Engineered) - Accuracy: {accuracy_eng:.4f}, F1: {f1_eng:.4f}, AUC: {auc_display_eng}\")\n",
    "        print(f\"   Training time: {training_time:.2f}s\")\n",
    "        print(f\"   ðŸ“ˆ F1 Improvement: {f1_improvement:+.2f}% | Accuracy Improvement: {accuracy_improvement:+.2f}%\")\n",
    "        print(f\"   Run ID: {run.info.run_id}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Feature-Engineered Training Complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Comprehensive Results Analysis ---\n",
    "\n",
    "print(\"ðŸ“Š CLASSIFICATION GAUNTLET RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create comprehensive comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "for model_name in models.keys():\n",
    "    # Baseline results\n",
    "    baseline = baseline_results[model_name]\n",
    "    engineered = engineered_results[model_name]\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model': f\"{model_name} (Baseline)\",\n",
    "        'Features': 'Standard',\n",
    "        'Accuracy': baseline['accuracy'],\n",
    "        'F1_Score': baseline['f1_score'],\n",
    "        'AUC_Score': baseline['auc_score'],\n",
    "        'Training_Time': baseline['training_time'],\n",
    "        'Feature_Count': X_train_scaled.shape[1]\n",
    "    })\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model': f\"{model_name} (Engineered)\",\n",
    "        'Features': 'Engineered',\n",
    "        'Accuracy': engineered['accuracy'],\n",
    "        'F1_Score': engineered['f1_score'],\n",
    "        'AUC_Score': engineered['auc_score'],\n",
    "        'Training_Time': engineered['training_time'],\n",
    "        'Feature_Count': X_train_eng_scaled.shape[1],\n",
    "        'F1_Improvement': engineered['f1_improvement'],\n",
    "        'Accuracy_Improvement': engineered['accuracy_improvement']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\nðŸ† COMPLETE PERFORMANCE COMPARISON\")\n",
    "print(\"-\" * 60)\n",
    "display_cols = ['Model', 'Features', 'Accuracy', 'F1_Score', 'AUC_Score', 'Training_Time', 'Feature_Count']\n",
    "print(comparison_df[display_cols].round(4).to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ“ˆ FEATURE ENGINEERING IMPACT\")\n",
    "print(\"-\" * 60)\n",
    "for model_name in models.keys():\n",
    "    eng_results = engineered_results[model_name]\n",
    "    base_results = baseline_results[model_name]\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  ðŸŽ¯ F1 Score: {base_results['f1_score']:.4f} â†’ {eng_results['f1_score']:.4f} ({eng_results['f1_improvement']:+.2f}%)\")\n",
    "    print(f\"  ðŸŽ¯ Accuracy: {base_results['accuracy']:.4f} â†’ {eng_results['accuracy']:.4f} ({eng_results['accuracy_improvement']:+.2f}%)\")\n",
    "    if eng_results['auc_score'] and base_results['auc_score']:\n",
    "        auc_improvement = ((eng_results['auc_score'] - base_results['auc_score']) / base_results['auc_score']) * 100\n",
    "        print(f\"  ðŸŽ¯ AUC Score: {base_results['auc_score']:.4f} â†’ {eng_results['auc_score']:.4f} ({auc_improvement:+.2f}%)\")\n",
    "\n",
    "# Identify champion models\n",
    "print(f\"\\nðŸ† CHAMPION MODELS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Best baseline model\n",
    "best_baseline = max(baseline_results.items(), key=lambda x: x[1]['f1_score'])\n",
    "print(f\"ðŸ¥‡ Best Baseline: {best_baseline[0]} (F1: {best_baseline[1]['f1_score']:.4f})\")\n",
    "\n",
    "# Best engineered model\n",
    "best_engineered = max(engineered_results.items(), key=lambda x: x[1]['f1_score'])\n",
    "print(f\"ðŸ¥‡ Best Engineered: {best_engineered[0]} (F1: {best_engineered[1]['f1_score']:.4f})\")\n",
    "\n",
    "# Overall champion\n",
    "overall_best = best_engineered if best_engineered[1]['f1_score'] > best_baseline[1]['f1_score'] else best_baseline\n",
    "print(f\"ðŸ‘‘ OVERALL CHAMPION: {overall_best[0]} ({'Engineered' if overall_best == best_engineered else 'Baseline'})\")\n",
    "\n",
    "# Feature engineering value\n",
    "avg_f1_improvement = np.mean([eng['f1_improvement'] for eng in engineered_results.values()])\n",
    "print(f\"\\nðŸ“Š FEATURE ENGINEERING VALUE\")\n",
    "print(f\"   Average F1 improvement: {avg_f1_improvement:+.2f}%\")\n",
    "print(f\"   Features added: {X_train_eng_scaled.shape[1] - X_train_scaled.shape[1]}\")\n",
    "print(f\"   Feature expansion: {X_train_scaled.shape[1]} â†’ {X_train_eng_scaled.shape[1]} features\")\n",
    "\n",
    "print(f\"\\nâœ… Classification Gauntlet Complete! Check MLflow UI at: {mlflow.get_tracking_uri()}\")\n",
    "print(\"ðŸŽ¯ All models registered and ready for deployment!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26ca2cc",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [2]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef4f4f",
   "metadata": {
    "papermill": {
     "duration": 0.00143,
     "end_time": "2025-09-16T20:23:29.906349",
     "exception": false,
     "start_time": "2025-09-16T20:23:29.904919",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Phase 4: The Second Classification Gauntlet (Kaggle Pump Data)\n",
    "\n",
    "**Objective**: Apply our classification pipeline to the **Kaggle Pump Sensor Data** to test the generalizability of our methods on a new, real-world tabular dataset.\n",
    "\n",
    "This notebook will:\n",
    "1. Load and preprocess the Kaggle pump sensor maintenance data\n",
    "2. Train baseline models (RandomForest, LightGBM) on cleaned data\n",
    "3. Apply advanced feature engineering techniques\n",
    "4. Re-train models on engineered features\n",
    "5. Compare performance and log all results to MLflow\n",
    "\n",
    "**Dataset**: `sensor_maintenance_data.csv` - Real-world pump sensor data with maintenance records\n",
    "**Target Variable**: `Operational Status` - Operational status of pump machinery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa187a54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T20:23:29.909371Z",
     "iopub.status.busy": "2025-09-16T20:23:29.909164Z",
     "iopub.status.idle": "2025-09-16T20:23:33.855146Z",
     "shell.execute_reply": "2025-09-16T20:23:33.854854Z"
    },
    "papermill": {
     "duration": 3.948276,
     "end_time": "2025-09-16T20:23:33.855694",
     "exception": false,
     "start_time": "2025-09-16T20:23:29.907418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir -p failed for path /.config/matplotlib: [Errno 13] Permission denied: '/.config'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-hcj3w22o because there was an issue with the default path (/.config/matplotlib); it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Tracking URI: http://mlflow:5000\n",
      "Phase 4: The Second Classification Gauntlet Starting...\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.lightgbm\n",
    "from datetime import datetime\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set MLflow tracking URI based on environment\n",
    "if os.getenv('DOCKER_ENV') == 'true':\n",
    "    mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "else:\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "print(\"MLflow Tracking URI:\", mlflow.get_tracking_uri())\n",
    "print(\"Phase 4: The Second Classification Gauntlet Starting...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70564b8e",
   "metadata": {
    "papermill": {
     "duration": 0.001211,
     "end_time": "2025-09-16T20:23:33.858367",
     "exception": false,
     "start_time": "2025-09-16T20:23:33.857156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset & Reproducibility Note\n",
    "This notebook uses the Kaggle Pump Sensor dataset tracked by DVC:\n",
    "- CSV: `data/kaggle_pump_sensor_data/sensor_maintenance_data.csv`\n",
    "- DVC pointer: `data/kaggle_pump_sensor_data.dvc`\n",
    "The next cell resolves the dataset path from the project root and will attempt `dvc pull` if the file is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b7043",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea61445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T20:23:33.861308Z",
     "iopub.status.busy": "2025-09-16T20:23:33.860905Z",
     "iopub.status.idle": "2025-09-16T20:23:34.044529Z",
     "shell.execute_reply": "2025-09-16T20:23:34.044163Z"
    },
    "papermill": {
     "duration": 0.185618,
     "end_time": "2025-09-16T20:23:34.044971",
     "exception": true,
     "start_time": "2025-09-16T20:23:33.859353",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Kaggle Pump Sensor Data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DATA_CSV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading Kaggle Pump Sensor Data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = pd.read_csv(\u001b[43mDATA_CSV\u001b[49m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.columns.tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'DATA_CSV' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 4.2: Data Loading & Preprocessing\n",
    "print(\"Loading Kaggle Pump Sensor Data...\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nTarget variable distribution:\")\n",
    "print(df['Operational Status'].value_counts())\n",
    "\n",
    "# Data preprocessing\n",
    "print(\"\\n=== Data Preprocessing ===\")\n",
    "\n",
    "# Handle missing values\n",
    "print(f\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Select relevant features for classification\n",
    "# Exclude IDs, timestamps, and text fields that aren't suitable for direct modeling\n",
    "numeric_features = [\n",
    "    'Voltage (V)', 'Current (A)', 'Temperature (°C)', 'Power (W)', \n",
    "    'Humidity (%)', 'Vibration (m/s²)', 'Repair Time (hrs)', \n",
    "    'Maintenance Costs (USD)', 'Ambient Temperature (°C)', \n",
    "    'Ambient Humidity (%)', 'X', 'Y', 'Z'\n",
    "],\n",
    "\n",
    "categorical_features = [\n",
    "    'Fault Status', 'Failure Type', 'Maintenance Type', \n",
    "    'Failure History', 'External Factors', 'Equipment Relationship', \n",
    "    'Equipment Criticality'\n",
    "],\n",
    "\n",
    "# Create feature matrix\n",
    "X_numeric = df[numeric_features].copy()\n",
    "X_categorical = df[categorical_features].copy()\n",
    "\n",
    "# Handle missing values in numeric features\n",
    "X_numeric = X_numeric.fillna(X_numeric.median())\n",
    "\n",
    "# Handle missing values in categorical features\n",
    "X_categorical = X_categorical.fillna('Unknown')\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    X_categorical[col] = le.fit_transform(X_categorical[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Combine features\n",
    "X = pd.concat([X_numeric, X_categorical], axis=1)\n",
    "\n",
    "# Prepare target variable\n",
    "y = df['Operational Status'].copy()\n",
    "print(f\"\\nTarget classes: {y.unique()}\")\n",
    "\n",
    "# Encode target variable\n",
    "target_encoder = LabelEncoder()\n",
    "y_encoded = target_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"Encoded target distribution:\")\n",
    "for i, class_name in enumerate(target_encoder.classes_):\n",
    "    count = sum(y_encoded == i)\n",
    "    print(f\"  {class_name}: {count} ({count/len(y_encoded)*100:.1f}%)\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Features used: {X.columns.tolist()}\")\n",
    "\n",
    "print(\"✅ Data preprocessing completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd4770",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resolve dataset path and ensure availability via DVC if needed\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "def project_root(start: Path = Path.cwd()) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / 'pyproject.toml').exists() or (p / '.dvc').exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "ROOT = project_root()\n",
    "DATA_CSV_REL = Path('data/kaggle_pump_sensor_data/sensor_maintenance_data.csv')\n",
    "DATA_DVC_REL = Path('data/kaggle_pump_sensor_data.dvc')\n",
    "DATA_CSV = ROOT / DATA_CSV_REL\n",
    "DATA_DVC = ROOT / DATA_DVC_REL\n",
    "\n",
    "print(f'Project root: {ROOT}')\n",
    "print(f'Dataset CSV: {DATA_CSV}')\n",
    "print(f'DVC pointer: {DATA_DVC}')\n",
    "\n",
    "def ensure_dataset():\n",
    "    if DATA_CSV.exists():\n",
    "        print('✅ Dataset CSV already present.')\n",
    "        return\n",
    "    # Attempt dvc pull at project root if .dvc dir exists\n",
    "    if (ROOT / '.dvc').exists():\n",
    "        try:\n",
    "            print('📦 Running dvc pull to fetch data artifacts...')\n",
    "            subprocess.run(['dvc', 'pull'], cwd=str(ROOT), check=True)\n",
    "        except Exception as e:\n",
    "            print(f'⚠️ dvc pull failed or not available: {e}')\n",
    "    else:\n",
    "        print('ℹ️ .dvc directory not found; skipping dvc pull.')\n",
    "\n",
    "    if not DATA_CSV.exists():\n",
    "        raise FileNotFoundError(f'Missing dataset: {DATA_CSV_REL}. If using DVC, run `dvc pull` at {ROOT}.')\n",
    "\n",
    "ensure_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5d4081",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 4.3 Part 1: Baseline Model Training\n",
    "print(\"=== Training Baseline Models ===\")\n",
    "\n",
    "# Set up MLflow experiment\n",
    "experiment_name = \"Classification Gauntlet (Kaggle Pump)\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Scale features for baseline models\n",
    "scaler_baseline = StandardScaler()\n",
    "X_train_scaled = scaler_baseline.fit_transform(X_train)\n",
    "X_test_scaled = scaler_baseline.transform(X_test)\n",
    "\n",
    "def train_and_log_model(model, model_name, X_train_data, X_test_data, y_train_data, y_test_data, \n",
    "                       feature_type=\"baseline\", run_name_suffix=\"\"):\n",
    "    \"\"\"Train model and log results to MLflow\"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"{model_name}_{feature_type}{run_name_suffix}\"):\n",
    "        # Train model\n",
    "        model.fit(X_train_data, y_train_data)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_data)\n",
    "        y_pred_proba = model.predict_proba(X_test_data)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test_data, y_pred)\n",
    "        \n",
    "        # For binary classification, use the positive class probabilities\n",
    "        n_classes = len(np.unique(y_test_data))\n",
    "        if n_classes == 2:\n",
    "            # Binary classification - use probabilities for the positive class\n",
    "            roc_auc = roc_auc_score(y_test_data, y_pred_proba[:, 1])\n",
    "        else:\n",
    "            # Multi-class classification - use ovr strategy\n",
    "            roc_auc = roc_auc_score(y_test_data, y_pred_proba, multi_class='ovr', average='macro')\n",
    "        \n",
    "        # Log parameters\n",
    "        if hasattr(model, 'get_params'):\n",
    "            for param, value in model.get_params().items():\n",
    "                mlflow.log_param(param, value)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "        \n",
    "        # Log feature information\n",
    "        mlflow.log_param(\"feature_type\", feature_type)\n",
    "        mlflow.log_param(\"n_features\", X_train_data.shape[1])\n",
    "        mlflow.log_param(\"n_samples_train\", X_train_data.shape[0])\n",
    "        mlflow.log_param(\"n_samples_test\", X_test_data.shape[0])\n",
    "        mlflow.log_param(\"n_classes\", n_classes)\n",
    "        \n",
    "        # Create and log classification report\n",
    "        class_report = classification_report(y_test_data, y_pred, \n",
    "                                           target_names=target_encoder.classes_)\n",
    "        \n",
    "        # Save classification report as artifact\n",
    "        report_path = f\"docs/ml/classification_report_{model_name}_{feature_type}.txt\"\n",
    "        os.makedirs(\"docs/ml\", exist_ok=True)\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(class_report)\n",
    "        mlflow.log_artifact(report_path)\n",
    "        \n",
    "        # Log model\n",
    "        if 'RandomForest' in model_name:\n",
    "            mlflow.sklearn.log_model(model, f\"{model_name}_{feature_type}\")\n",
    "        elif 'LightGBM' in model_name:\n",
    "            mlflow.lightgbm.log_model(model, f\"{model_name}_{feature_type}\")\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(model, f\"{model_name}_{feature_type}\")\n",
    "        \n",
    "        print(f\"{model_name} ({feature_type}):\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  ROC-AUC: {roc_auc:.4f}\")\n",
    "        print(f\"  Classes: {n_classes}\")\n",
    "        print(f\"  Classification Report:\\n{class_report}\")\n",
    "        \n",
    "        return accuracy, roc_auc, model\n",
    "\n",
    "# Train baseline models\n",
    "print(\"Training baseline models on original features...\")\n",
    "\n",
    "# 1. RandomForest Baseline\n",
    "rf_baseline = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_acc_base, rf_auc_base, rf_model_base = train_and_log_model(\n",
    "    rf_baseline, \"RandomForest\", X_train_scaled, X_test_scaled, y_train, y_test, \"baseline\"\n",
    ")\n",
    "\n",
    "# 2. LightGBM Baseline\n",
    "lgb_baseline = lgb.LGBMClassifier(n_estimators=100, random_state=42, verbosity=-1)\n",
    "lgb_acc_base, lgb_auc_base, lgb_model_base = train_and_log_model(\n",
    "    lgb_baseline, \"LightGBM\", X_train_scaled, X_test_scaled, y_train, y_test, \"baseline\"\n",
    ")\n",
    "\n",
    "print(\"✅ Baseline model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2f7f9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 4.3 Part 2: Advanced Feature Engineering\n",
    "print(\"=== Advanced Feature Engineering ===\")\n",
    "\n",
    "# Create enhanced feature set\n",
    "X_train_engineered = X_train.copy()\n",
    "X_test_engineered = X_test.copy()\n",
    "\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "\n",
    "# 1. Power-related features\n",
    "print(\"Creating power and efficiency features...\")\n",
    "X_train_engineered['Power_Efficiency'] = X_train_engineered['Power (W)'] / (X_train_engineered['Voltage (V)'] * X_train_engineered['Current (A)'] + 1e-8)\n",
    "X_test_engineered['Power_Efficiency'] = X_test_engineered['Power (W)'] / (X_test_engineered['Voltage (V)'] * X_test_engineered['Current (A)'] + 1e-8)\n",
    "\n",
    "X_train_engineered['Voltage_Current_Ratio'] = X_train_engineered['Voltage (V)'] / (X_train_engineered['Current (A)'] + 1e-8)\n",
    "X_test_engineered['Voltage_Current_Ratio'] = X_test_engineered['Voltage (V)'] / (X_test_engineered['Current (A)'] + 1e-8)\n",
    "\n",
    "# 2. Temperature-related features\n",
    "print(\"Creating temperature differential features...\")\n",
    "X_train_engineered['Temp_Differential'] = X_train_engineered['Temperature (°C)'] - X_train_engineered['Ambient Temperature (°C)']\n",
    "X_test_engineered['Temp_Differential'] = X_test_engineered['Temperature (°C)'] - X_test_engineered['Ambient Temperature (°C)']\n",
    "\n",
    "X_train_engineered['Temp_Humidity_Interaction'] = X_train_engineered['Temperature (°C)'] * X_train_engineered['Humidity (%)']\n",
    "X_test_engineered['Temp_Humidity_Interaction'] = X_test_engineered['Temperature (°C)'] * X_test_engineered['Humidity (%)']\n",
    "\n",
    "# 3. Environmental stress indicators\n",
    "print(\"Creating environmental stress features...\")\n",
    "X_train_engineered['Humidity_Differential'] = X_train_engineered['Humidity (%)'] - X_train_engineered['Ambient Humidity (%)']\n",
    "X_test_engineered['Humidity_Differential'] = X_test_engineered['Humidity (%)'] - X_test_engineered['Ambient Humidity (%)']\n",
    "\n",
    "X_train_engineered['Environmental_Stress'] = (\n",
    "    abs(X_train_engineered['Temp_Differential']) + \n",
    "    abs(X_train_engineered['Humidity_Differential']) + \n",
    "    X_train_engineered['Vibration (m/s²)'] * 10\n",
    ")\n",
    "X_test_engineered['Environmental_Stress'] = (\n",
    "    abs(X_test_engineered['Temp_Differential']) + \n",
    "    abs(X_test_engineered['Humidity_Differential']) + \n",
    "    X_test_engineered['Vibration (m/s²)'] * 10\n",
    ")\n",
    "\n",
    "# 4. Maintenance and cost features\n",
    "print(\"Creating maintenance-related features...\")\n",
    "X_train_engineered['Cost_Per_Hour'] = X_train_engineered['Maintenance Costs (USD)'] / (X_train_engineered['Repair Time (hrs)'] + 1e-8)\n",
    "X_test_engineered['Cost_Per_Hour'] = X_test_engineered['Maintenance Costs (USD)'] / (X_test_engineered['Repair Time (hrs)'] + 1e-8)\n",
    "\n",
    "# 5. Spatial features (X, Y, Z coordinates)\n",
    "print(\"Creating spatial features...\")\n",
    "X_train_engineered['Spatial_Distance'] = np.sqrt(X_train_engineered['X']**2 + X_train_engineered['Y']**2 + X_train_engineered['Z']**2)\n",
    "X_test_engineered['Spatial_Distance'] = np.sqrt(X_test_engineered['X']**2 + X_test_engineered['Y']**2 + X_test_engineered['Z']**2)\n",
    "\n",
    "# 6. Vibration intensity features\n",
    "print(\"Creating vibration-based features...\")\n",
    "X_train_engineered['Vibration_Power_Ratio'] = X_train_engineered['Vibration (m/s²)'] / (X_train_engineered['Power (W)'] + 1e-8)\n",
    "X_test_engineered['Vibration_Power_Ratio'] = X_test_engineered['Vibration (m/s²)'] / (X_test_engineered['Power (W)'] + 1e-8)\n",
    "\n",
    "# 7. Statistical binning for key continuous variables\n",
    "print(\"Creating binned features...\")\n",
    "# Temperature bins\n",
    "temp_bins = pd.qcut(X_train_engineered['Temperature (°C)'], q=4, labels=['Low', 'Medium', 'High', 'Very_High'])\n",
    "X_train_engineered['Temp_Bin'] = pd.qcut(X_train_engineered['Temperature (°C)'], q=4, labels=[0, 1, 2, 3])\n",
    "# Apply same bins to test set\n",
    "temp_bin_edges = pd.qcut(X_train_engineered['Temperature (°C)'], q=4, retbins=True)[1]\n",
    "X_test_engineered['Temp_Bin'] = pd.cut(X_test_engineered['Temperature (°C)'], bins=temp_bin_edges, labels=[0, 1, 2, 3], include_lowest=True)\n",
    "\n",
    "# Vibration bins\n",
    "X_train_engineered['Vibration_Bin'] = pd.qcut(X_train_engineered['Vibration (m/s²)'], q=3, labels=[0, 1, 2])\n",
    "vibration_bin_edges = pd.qcut(X_train_engineered['Vibration (m/s²)'], q=3, retbins=True)[1]\n",
    "X_test_engineered['Vibration_Bin'] = pd.cut(X_test_engineered['Vibration (m/s²)'], bins=vibration_bin_edges, labels=[0, 1, 2], include_lowest=True)\n",
    "\n",
    "# Handle any NaN values created during binning\n",
    "X_train_engineered = X_train_engineered.fillna(0)\n",
    "X_test_engineered = X_test_engineered.fillna(0)\n",
    "\n",
    "print(f\"Engineered features: {X_train_engineered.shape[1]}\")\n",
    "print(f\"New features added: {X_train_engineered.shape[1] - X_train.shape[1]}\")\n",
    "\n",
    "# List the new features created\n",
    "new_features = [col for col in X_train_engineered.columns if col not in X_train.columns]\n",
    "print(f\"New feature names: {new_features}\")\n",
    "\n",
    "# Scale the engineered features\n",
    "scaler_engineered = StandardScaler()\n",
    "X_train_engineered_scaled = scaler_engineered.fit_transform(X_train_engineered)\n",
    "X_test_engineered_scaled = scaler_engineered.transform(X_test_engineered)\n",
    "\n",
    "print(\"✅ Feature engineering completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b0142",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 4.3 Part 3: Feature-Engineered Model Training\n",
    "print(\"=== Training Models on Engineered Features ===\")\n",
    "\n",
    "# Train models on engineered features\n",
    "print(\"Training models with advanced feature engineering...\")\n",
    "\n",
    "# 1. RandomForest with Engineered Features\n",
    "rf_engineered = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_acc_eng, rf_auc_eng, rf_model_eng = train_and_log_model(\n",
    "    rf_engineered, \"RandomForest\", X_train_engineered_scaled, X_test_engineered_scaled, \n",
    "    y_train, y_test, \"engineered\"\n",
    ")\n",
    "\n",
    "# 2. LightGBM with Engineered Features\n",
    "lgb_engineered = lgb.LGBMClassifier(n_estimators=100, random_state=42, verbosity=-1)\n",
    "lgb_acc_eng, lgb_auc_eng, lgb_model_eng = train_and_log_model(\n",
    "    lgb_engineered, \"LightGBM\", X_train_engineered_scaled, X_test_engineered_scaled, \n",
    "    y_train, y_test, \"engineered\"\n",
    ")\n",
    "\n",
    "# Performance Comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 4 RESULTS SUMMARY - KAGGLE PUMP CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_summary = pd.DataFrame({\n",
    "    'Model': ['RandomForest', 'RandomForest', 'LightGBM', 'LightGBM'],\n",
    "    'Feature_Type': ['Baseline', 'Engineered', 'Baseline', 'Engineered'],\n",
    "    'Accuracy': [rf_acc_base, rf_acc_eng, lgb_acc_base, lgb_acc_eng],\n",
    "    'ROC_AUC': [rf_auc_base, rf_auc_eng, lgb_auc_base, lgb_auc_eng],\n",
    "    'Features_Count': [X_train.shape[1], X_train_engineered.shape[1], \n",
    "                      X_train.shape[1], X_train_engineered.shape[1]]\n",
    "})\n",
    "\n",
    "print(results_summary.round(4))\n",
    "\n",
    "# Calculate improvements\n",
    "rf_acc_improvement = ((rf_acc_eng - rf_acc_base) / rf_acc_base) * 100\n",
    "rf_auc_improvement = ((rf_auc_eng - rf_auc_base) / rf_auc_base) * 100\n",
    "lgb_acc_improvement = ((lgb_acc_eng - lgb_acc_base) / lgb_acc_base) * 100\n",
    "lgb_auc_improvement = ((lgb_auc_eng - lgb_auc_base) / lgb_auc_base) * 100\n",
    "\n",
    "print(f\"\\nPerformance Improvements:\")\n",
    "print(f\"RandomForest - Accuracy: {rf_acc_improvement:+.2f}%, ROC-AUC: {rf_auc_improvement:+.2f}%\")\n",
    "print(f\"LightGBM     - Accuracy: {lgb_acc_improvement:+.2f}%, ROC-AUC: {lgb_auc_improvement:+.2f}%\")\n",
    "\n",
    "# Determine champion model\n",
    "champion_idx = results_summary['Accuracy'].idxmax()\n",
    "champion_model = results_summary.iloc[champion_idx]\n",
    "\n",
    "print(f\"\\n🏆 CHAMPION MODEL:\")\n",
    "print(f\"   {champion_model['Model']} ({champion_model['Feature_Type']})\")\n",
    "print(f\"   Accuracy: {champion_model['Accuracy']:.4f}\")\n",
    "print(f\"   ROC-AUC: {champion_model['ROC_AUC']:.4f}\")\n",
    "print(f\"   Features: {champion_model['Features_Count']}\")\n",
    "\n",
    "# Feature importance analysis for champion model\n",
    "if champion_model['Model'] == 'RandomForest' and champion_model['Feature_Type'] == 'Baseline':\n",
    "    champion_trained_model = rf_model_base\n",
    "    feature_names = X.columns\n",
    "elif champion_model['Model'] == 'RandomForest' and champion_model['Feature_Type'] == 'Engineered':\n",
    "    champion_trained_model = rf_model_eng\n",
    "    feature_names = X_train_engineered.columns\n",
    "elif champion_model['Model'] == 'LightGBM' and champion_model['Feature_Type'] == 'Baseline':\n",
    "    champion_trained_model = lgb_model_base\n",
    "    feature_names = X.columns\n",
    "else:  # LightGBM Engineered\n",
    "    champion_trained_model = lgb_model_eng\n",
    "    feature_names = X_train_engineered.columns\n",
    "\n",
    "# Get feature importance\n",
    "if hasattr(champion_trained_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': champion_trained_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Most Important Features ({champion_model['Model']} {champion_model['Feature_Type']}):\")\n",
    "    print(feature_importance.head(10).round(4))\n",
    "    \n",
    "    # Save feature importance plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 15 Feature Importance - {champion_model[\"Model\"]} ({champion_model[\"Feature_Type\"]})')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    importance_plot_path = \"docs/ml/pump_feature_importance.png\"\n",
    "    plt.savefig(importance_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Log the plot as an artifact in the last MLflow run\n",
    "    mlflow.log_artifact(importance_plot_path)\n",
    "\n",
    "# Save results summary\n",
    "summary_path = \"docs/ml/pump_classification_summary.csv\"\n",
    "results_summary.to_csv(summary_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Phase 4: The Second Classification Gauntlet COMPLETED!\")\n",
    "print(f\"📊 Results summary saved to: {summary_path}\")\n",
    "print(f\"🎯 Dataset: Kaggle Pump Sensor Data ({df.shape[0]} samples, {len(target_encoder.classes_)} classes)\")\n",
    "print(f\"🧪 Models trained: 4 (2 algorithms × 2 feature sets)\")\n",
    "print(f\"📈 Best performance: {champion_model['Accuracy']:.1%} accuracy with {champion_model['Model']} ({champion_model['Feature_Type']})\")\n",
    "\n",
    "# MLflow experiment info\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "print(f\"🔬 MLflow Experiment: '{experiment_name}' (ID: {experiment.experiment_id})\")\n",
    "print(f\"🌐 View results at: {mlflow.get_tracking_uri()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.432144,
   "end_time": "2025-09-16T20:23:34.661204",
   "environment_variables": {},
   "exception": true,
   "input_path": "notebooks/08_pump_classification.ipynb",
   "output_path": "notebooks/08_pump_classification_output.ipynb",
   "parameters": {},
   "start_time": "2025-09-16T20:23:29.229060",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
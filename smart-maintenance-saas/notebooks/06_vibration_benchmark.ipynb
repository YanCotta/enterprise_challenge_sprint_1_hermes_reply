{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: The Vibration Gauntlet (NASA Bearing Dataset)\n",
    "\n",
    "This notebook processes raw vibration signal data from the NASA bearing dataset to build unsupervised anomaly detection models.\n",
    "\n",
    "## Objectives:\n",
    "1. Load and process NASA bearing vibration time-series data\n",
    "2. Extract statistical and frequency-domain features from raw signals\n",
    "3. Train unsupervised anomaly detection models (IsolationForest, OneClassSVM)\n",
    "4. Register models in MLflow for production deployment\n",
    "\n",
    "## Dataset: NASA Bearing Dataset\n",
    "- **Source**: IMS Bearing Dataset from NASA Prognostics Center\n",
    "- **Type**: Vibration time-series data from accelerometers\n",
    "- **Structure**: 8-channel accelerometer readings (4 bearings × 2 sensors each)\n",
    "- **Sampling**: 20kHz sampling frequency, 1-second windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import os\n",
    "import glob\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.fft import fft, fftfreq\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# MLflow configuration\n",
    "tracking_uri = \"http://mlflow:5000\" if os.getenv(\"DOCKER_ENV\") == \"true\" else \"http://localhost:5000\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "mlflow.set_experiment(\"Vibration Gauntlet (NASA)\")\n",
    "\n",
    "print(f\"MLflow tracking URI set to: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs('docs/ml', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bearing_data(file_path):\n",
    "    \"\"\"\n",
    "    Load NASA bearing data file containing 8-channel vibration readings.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Vibration data with shape (n_samples, 8_channels)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the data file - each row contains 8 accelerometer readings\n",
    "        data = np.loadtxt(file_path)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_vibration_features(signal_window, sampling_freq=20000):\n",
    "    \"\"\"\n",
    "    Extract comprehensive features from a vibration signal window.\n",
    "    \n",
    "    Parameters:\n",
    "        signal_window (np.array): 1D vibration signal\n",
    "        sampling_freq (int): Sampling frequency in Hz\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing extracted features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Statistical Features\n",
    "    features['rms'] = np.sqrt(np.mean(signal_window**2))  # Root Mean Square - overall energy\n",
    "    features['peak_to_peak'] = np.max(signal_window) - np.min(signal_window)  # Peak-to-peak amplitude\n",
    "    features['kurtosis'] = kurtosis(signal_window)  # Kurtosis - indicates impulsiveness (bearing defects)\n",
    "    features['skewness'] = skew(signal_window)  # Skewness - indicates asymmetry\n",
    "    features['std'] = np.std(signal_window)  # Standard deviation\n",
    "    features['mean'] = np.mean(signal_window)  # Mean value\n",
    "    features['crest_factor'] = np.max(np.abs(signal_window)) / features['rms']  # Crest factor\n",
    "    \n",
    "    # Frequency Domain Features (FFT)\n",
    "    fft_values = np.abs(fft(signal_window))\n",
    "    freqs = fftfreq(len(signal_window), 1/sampling_freq)\n",
    "    \n",
    "    # Only consider positive frequencies\n",
    "    positive_freq_idx = freqs > 0\n",
    "    fft_positive = fft_values[positive_freq_idx]\n",
    "    freqs_positive = freqs[positive_freq_idx]\n",
    "    \n",
    "    # Find dominant frequency\n",
    "    dominant_freq_idx = np.argmax(fft_positive)\n",
    "    features['dominant_freq'] = freqs_positive[dominant_freq_idx]\n",
    "    features['dominant_freq_amplitude'] = fft_positive[dominant_freq_idx]\n",
    "    \n",
    "    # Spectral centroid (center of mass of spectrum)\n",
    "    features['spectral_centroid'] = np.sum(freqs_positive * fft_positive) / np.sum(fft_positive)\n",
    "    \n",
    "    # High frequency content (above 1kHz)\n",
    "    high_freq_mask = freqs_positive > 1000\n",
    "    features['high_freq_energy'] = np.sum(fft_positive[high_freq_mask]**2) / np.sum(fft_positive**2)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def process_bearing_file(file_path, window_size=2048):\n",
    "    \"\"\"\n",
    "    Process a single bearing data file and extract features from multiple windows.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to bearing data file\n",
    "        window_size (int): Size of sliding window for feature extraction\n",
    "    \n",
    "    Returns:\n",
    "        list: List of feature dictionaries\n",
    "    \"\"\"\n",
    "    data = load_bearing_data(file_path)\n",
    "    if data is None:\n",
    "        return []\n",
    "    \n",
    "    # Extract filename for timestamp parsing\n",
    "    filename = os.path.basename(file_path)\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    # Process each sensor channel (8 channels total)\n",
    "    for channel in range(data.shape[1]):\n",
    "        channel_data = data[:, channel]\n",
    "        \n",
    "        # Extract features from overlapping windows\n",
    "        step_size = window_size // 2  # 50% overlap\n",
    "        for start_idx in range(0, len(channel_data) - window_size, step_size):\n",
    "            window = channel_data[start_idx:start_idx + window_size]\n",
    "            \n",
    "            # Extract features\n",
    "            features = extract_vibration_features(window)\n",
    "            \n",
    "            # Add metadata\n",
    "            features['filename'] = filename\n",
    "            features['channel'] = channel\n",
    "            features['window_start'] = start_idx\n",
    "            features['bearing_id'] = f\"bearing_{(channel // 2) + 1}\"  # 2 sensors per bearing\n",
    "            features['sensor_position'] = 'horizontal' if channel % 2 == 0 else 'vertical'\n",
    "            \n",
    "            features_list.append(features)\n",
    "    \n",
    "    return features_list\n",
    "\n",
    "# Test the functions with a sample file\n",
    "print(\"Testing signal processing functions...\")\n",
    "\n",
    "# Get the first available data file\n",
    "data_dir = \"data/nasa_bearing_dataset/4. Bearings/1st_test\"\n",
    "sample_files = glob.glob(os.path.join(data_dir, \"*\"))[:3]  # Test with first 3 files\n",
    "\n",
    "if sample_files:\n",
    "    print(f\"Processing sample files: {len(sample_files)} files\")\n",
    "    \n",
    "    # Process first file to verify functionality\n",
    "    sample_features = process_bearing_file(sample_files[0], window_size=1024)  # Smaller window for testing\n",
    "    \n",
    "    print(f\"Extracted {len(sample_features)} feature vectors from first file\")\n",
    "    if sample_features:\n",
    "        print(\"Sample features:\")\n",
    "        for key, value in list(sample_features[0].items())[:8]:\n",
    "            print(f\"  {key}: {value:.4f}\" if isinstance(value, (int, float)) else f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"No data files found. Please check the data directory path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process all bearing data files\n",
    "print(\"=== Phase 2: Vibration Signal Processing ===\")\n",
    "\n",
    "# Define data paths\n",
    "nasa_data_dir = \"data/nasa_bearing_dataset/4. Bearings/1st_test\"\n",
    "all_files = sorted(glob.glob(os.path.join(nasa_data_dir, \"*\")))\n",
    "\n",
    "print(f\"Found {len(all_files)} bearing data files\")\n",
    "\n",
    "# Process a subset of files for this demonstration (first 20 files)\n",
    "# In production, you would process all files\n",
    "sample_files = all_files[:20]\n",
    "print(f\"Processing {len(sample_files)} files for feature extraction...\")\n",
    "\n",
    "# Extract features from all files\n",
    "all_features = []\n",
    "window_size = 2048  # 2048 samples ≈ 0.1 seconds at 20kHz\n",
    "\n",
    "for i, file_path in enumerate(sample_files):\n",
    "    if i % 5 == 0:  # Progress indicator\n",
    "        print(f\"Processing file {i+1}/{len(sample_files)}: {os.path.basename(file_path)}\")\n",
    "    \n",
    "    file_features = process_bearing_file(file_path, window_size=window_size)\n",
    "    all_features.extend(file_features)\n",
    "\n",
    "# Convert to DataFrame\n",
    "print(f\"Creating feature dataset with {len(all_features)} feature vectors...\")\n",
    "features_df = pd.DataFrame(all_features)\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"\\nFeature dataset shape: {features_df.shape}\")\n",
    "print(f\"Number of unique files: {features_df['filename'].nunique()}\")\n",
    "print(f\"Number of bearings: {features_df['bearing_id'].nunique()}\")\n",
    "print(f\"Sensor positions: {features_df['sensor_position'].unique()}\")\n",
    "\n",
    "# Statistical summary of key features\n",
    "feature_cols = ['rms', 'peak_to_peak', 'kurtosis', 'skewness', 'crest_factor', \n",
    "                'dominant_freq', 'spectral_centroid', 'high_freq_energy']\n",
    "\n",
    "print(f\"\\nKey vibration features summary:\")\n",
    "print(features_df[feature_cols].describe())\n",
    "\n",
    "# Create feature correlation matrix\n",
    "correlation_matrix = features_df[feature_cols].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Vibration Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('docs/ml/vibration_feature_correlation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Display sample of the processed data\n",
    "print(f\"\\nSample of processed vibration features:\")\n",
    "print(features_df[['bearing_id', 'sensor_position', 'rms', 'kurtosis', 'dominant_freq']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for anomaly detection training\n",
    "print(\"=== Anomaly Detection Model Training ===\")\n",
    "\n",
    "# Select feature columns for modeling\n",
    "feature_columns = ['rms', 'peak_to_peak', 'kurtosis', 'skewness', 'std', 'crest_factor',\n",
    "                   'dominant_freq', 'dominant_freq_amplitude', 'spectral_centroid', 'high_freq_energy']\n",
    "\n",
    "X = features_df[feature_columns].copy()\n",
    "\n",
    "# Handle any infinite or NaN values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "print(f\"Training data shape: {X.shape}\")\n",
    "print(f\"Features used: {feature_columns}\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define models to train\n",
    "models = {\n",
    "    'IsolationForest': IsolationForest(contamination=0.1, random_state=42, n_estimators=100),\n",
    "    'OneClassSVM': OneClassSVM(nu=0.1, kernel='rbf', gamma='scale')\n",
    "}\n",
    "\n",
    "# Train models and log to MLflow\n",
    "model_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"Vibration_{model_name}\") as run:\n",
    "        # Train model\n",
    "        model.fit(X_scaled)\n",
    "        \n",
    "        # Generate predictions (-1 for anomaly, 1 for normal)\n",
    "        predictions = model.predict(X_scaled)\n",
    "        anomaly_scores = model.score_samples(X_scaled) if hasattr(model, 'score_samples') else model.decision_function(X_scaled)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        n_anomalies = np.sum(predictions == -1)\n",
    "        anomaly_rate = n_anomalies / len(predictions)\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params(model.get_params())\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"anomaly_rate\", anomaly_rate)\n",
    "        mlflow.log_metric(\"n_anomalies\", n_anomalies)\n",
    "        mlflow.log_metric(\"n_normal\", len(predictions) - n_anomalies)\n",
    "        mlflow.log_metric(\"mean_anomaly_score\", np.mean(anomaly_scores))\n",
    "        mlflow.log_metric(\"std_anomaly_score\", np.std(anomaly_scores))\n",
    "        \n",
    "        # Create anomaly score distribution plot\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(anomaly_scores[predictions == 1], bins=50, alpha=0.7, label='Normal', color='blue')\n",
    "        plt.hist(anomaly_scores[predictions == -1], bins=50, alpha=0.7, label='Anomaly', color='red')\n",
    "        plt.xlabel('Anomaly Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'{model_name} - Anomaly Score Distribution')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Create scatter plot of first two features colored by anomaly prediction\n",
    "        plt.subplot(1, 2, 2)\n",
    "        normal_mask = predictions == 1\n",
    "        anomaly_mask = predictions == -1\n",
    "        \n",
    "        plt.scatter(X.iloc[normal_mask, 0], X.iloc[normal_mask, 1], \n",
    "                   c='blue', alpha=0.6, s=20, label='Normal')\n",
    "        plt.scatter(X.iloc[anomaly_mask, 0], X.iloc[anomaly_mask, 1], \n",
    "                   c='red', alpha=0.8, s=20, label='Anomaly')\n",
    "        plt.xlabel(feature_columns[0])\n",
    "        plt.ylabel(feature_columns[1])\n",
    "        plt.title(f'{model_name} - Anomaly Detection Results')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_path = f'docs/ml/vibration_{model_name.lower()}_results.png'\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Log plot as artifact\n",
    "        mlflow.log_artifact(plot_path)\n",
    "        \n",
    "        # Log feature importance if available\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': feature_columns,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            feature_importance.to_csv(f'docs/ml/{model_name.lower()}_feature_importance.csv', index=False)\n",
    "            mlflow.log_artifact(f'docs/ml/{model_name.lower()}_feature_importance.csv')\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=model,\n",
    "            artifact_path=\"model\",\n",
    "            registered_model_name=f\"vibration_anomaly_{model_name.lower()}\"\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        model_results[model_name] = {\n",
    "            'model': model,\n",
    "            'anomaly_rate': anomaly_rate,\n",
    "            'n_anomalies': n_anomalies,\n",
    "            'mean_score': np.mean(anomaly_scores),\n",
    "            'std_score': np.std(anomaly_scores),\n",
    "            'run_id': run.info.run_id\n",
    "        }\n",
    "        \n",
    "        print(f\"  - Anomaly rate: {anomaly_rate:.3f}\")\n",
    "        print(f\"  - Anomalies detected: {n_anomalies}/{len(predictions)}\")\n",
    "        print(f\"  - Mean anomaly score: {np.mean(anomaly_scores):.4f}\")\n",
    "        print(f\"  - MLflow run ID: {run.info.run_id}\")\n",
    "\n",
    "# Summary of results\n",
    "print(f\"\\n=== Vibration Gauntlet Results Summary ===\")\n",
    "for model_name, results in model_results.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  - Anomaly Detection Rate: {results['anomaly_rate']:.3f}\")\n",
    "    print(f\"  - Anomalies Found: {results['n_anomalies']} out of {len(X)} samples\")\n",
    "    print(f\"  - Anomaly Score Range: {results['mean_score']:.4f} ± {results['std_score']:.4f}\")\n",
    "    \n",
    "print(f\"\\nAll models have been successfully registered in MLflow Model Registry!\")\n",
    "print(f\"View results at: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Analysis and Insights\n",
    "print(\"=== Advanced Vibration Analysis ===\")\n",
    "\n",
    "# Analyze anomaly patterns by bearing and sensor position\n",
    "anomaly_df = features_df.copy()\n",
    "anomaly_df['predictions_isolation'] = models['IsolationForest'].predict(X_scaled)\n",
    "anomaly_df['predictions_svm'] = models['OneClassSVM'].predict(X_scaled)\n",
    "\n",
    "# Anomaly distribution by bearing\n",
    "bearing_anomaly_summary = pd.DataFrame({\n",
    "    'bearing_id': anomaly_df['bearing_id'],\n",
    "    'isolation_forest_anomaly': anomaly_df['predictions_isolation'] == -1,\n",
    "    'oneclass_svm_anomaly': anomaly_df['predictions_svm'] == -1\n",
    "}).groupby('bearing_id').agg({\n",
    "    'isolation_forest_anomaly': ['count', 'sum', 'mean'],\n",
    "    'oneclass_svm_anomaly': ['count', 'sum', 'mean']\n",
    "}).round(3)\n",
    "\n",
    "print(\"Anomaly Detection by Bearing:\")\n",
    "print(bearing_anomaly_summary)\n",
    "\n",
    "# Sensor position analysis\n",
    "sensor_anomaly_summary = pd.DataFrame({\n",
    "    'sensor_position': anomaly_df['sensor_position'],\n",
    "    'isolation_forest_anomaly': anomaly_df['predictions_isolation'] == -1,\n",
    "    'oneclass_svm_anomaly': anomaly_df['predictions_svm'] == -1\n",
    "}).groupby('sensor_position').agg({\n",
    "    'isolation_forest_anomaly': ['count', 'sum', 'mean'],\n",
    "    'oneclass_svm_anomaly': ['count', 'sum', 'mean']\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\nAnomaly Detection by Sensor Position:\")\n",
    "print(sensor_anomaly_summary)\n",
    "\n",
    "# Feature importance analysis for anomalies\n",
    "print(\"\\n=== Feature Analysis for Detected Anomalies ===\")\n",
    "\n",
    "# Compare feature values between normal and anomaly samples\n",
    "anomaly_mask_iso = anomaly_df['predictions_isolation'] == -1\n",
    "normal_mask_iso = anomaly_df['predictions_isolation'] == 1\n",
    "\n",
    "feature_comparison = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'normal_mean': X[normal_mask_iso].mean(),\n",
    "    'anomaly_mean': X[anomaly_mask_iso].mean(),\n",
    "    'difference': X[anomaly_mask_iso].mean() - X[normal_mask_iso].mean(),\n",
    "    'ratio': X[anomaly_mask_iso].mean() / X[normal_mask_iso].mean()\n",
    "}).round(4)\n",
    "\n",
    "feature_comparison['abs_difference'] = abs(feature_comparison['difference'])\n",
    "feature_comparison = feature_comparison.sort_values('abs_difference', ascending=False)\n",
    "\n",
    "print(\"Feature comparison (Normal vs. Anomalous samples) - IsolationForest:\")\n",
    "print(feature_comparison[['feature', 'normal_mean', 'anomaly_mean', 'difference', 'ratio']])\n",
    "\n",
    "# Key insights about vibration patterns\n",
    "print(f\"\\n=== Key Insights from Vibration Analysis ===\")\n",
    "print(f\"1. Dataset Coverage:\")\n",
    "print(f\"   - Processed {len(sample_files)} files with {len(features_df)} feature windows\")\n",
    "print(f\"   - Analyzed {features_df['bearing_id'].nunique()} bearings with {window_size} sample windows\")\n",
    "print(f\"   - Frequency resolution: {20000/window_size:.1f} Hz per bin\")\n",
    "\n",
    "print(f\"\\n2. Signal Characteristics:\")\n",
    "most_important_features = feature_comparison.head(3)['feature'].tolist()\n",
    "print(f\"   - Most discriminative features: {', '.join(most_important_features)}\")\n",
    "print(f\"   - RMS values range: {X['rms'].min():.4f} to {X['rms'].max():.4f}\")\n",
    "print(f\"   - Dominant frequencies: {X['dominant_freq'].min():.0f} to {X['dominant_freq'].max():.0f} Hz\")\n",
    "\n",
    "print(f\"\\n3. Anomaly Detection Performance:\")\n",
    "for model_name, results in model_results.items():\n",
    "    print(f\"   - {model_name}: {results['anomaly_rate']:.1%} anomaly rate\")\n",
    "\n",
    "print(f\"\\n4. Industrial Significance:\")\n",
    "print(f\"   - Kurtosis > 3 may indicate bearing defects (impulsive signals)\")\n",
    "print(f\"   - High-frequency energy indicates surface roughness or defects\")\n",
    "print(f\"   - Crest factor > 3-4 suggests intermittent impacts\")\n",
    "\n",
    "# Save comprehensive results\n",
    "results_summary = {\n",
    "    'experiment_name': 'Vibration Gauntlet (NASA)',\n",
    "    'dataset': 'NASA IMS Bearing Dataset - 1st Test',\n",
    "    'files_processed': len(sample_files),\n",
    "    'total_windows': len(features_df),\n",
    "    'window_size': window_size,\n",
    "    'sampling_frequency': 20000,\n",
    "    'features_extracted': len(feature_columns),\n",
    "    'models_trained': list(model_results.keys()),\n",
    "    'mlflow_uri': tracking_uri\n",
    "}\n",
    "\n",
    "# Log summary to file\n",
    "import json\n",
    "with open('docs/ml/vibration_gauntlet_summary.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n=== Phase 2: Vibration Gauntlet COMPLETE ===\")\n",
    "print(f\"✅ Successfully processed NASA bearing vibration data\")\n",
    "print(f\"✅ Extracted {len(feature_columns)} signal processing features\")  \n",
    "print(f\"✅ Trained and registered {len(models)} anomaly detection models\")\n",
    "print(f\"✅ All models available in MLflow Model Registry\")\n",
    "print(f\"✅ Ready for Phase 3: Audio Gauntlet!\")\n",
    "\n",
    "# Display final dataset info\n",
    "print(f\"\\nFinal dataset shape: {features_df.shape}\")\n",
    "print(f\"Feature columns: {feature_columns}\")\n",
    "print(f\"MLflow tracking URI: {tracking_uri}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

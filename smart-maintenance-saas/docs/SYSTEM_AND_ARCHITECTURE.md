# Smart Maintenance SaaS - System and Architecture

üáßüá∑ **[Clique aqui para ler em Portugu√™s](#-smart-maintenance-saas---sistema-e-arquitetura-portugu√™s)** | üá∫üá∏ **English Version Below**

## üìö Documentation Navigation

This document is part of the Smart Maintenance SaaS documentation suite. For complete system understanding, please also refer to:

- **[Backend README](../README.md)** - Docker deployment and getting started guide
- **[Deployment Status](./DEPLOYMENT_STATUS.md)** - Current deployment status and container information
- **[API Documentation](./api.md)** - Complete REST API reference and usage examples  
- **[Performance Baseline](./PERFORMANCE_BASELINE.md)** - Load testing results and performance metrics baseline
- **[Load Testing Instructions](./LOAD_TESTING_INSTRUCTIONS.md)** - Comprehensive guide for running performance tests
- **[System Screenshots](./SYSTEM_SCREENSHOTS.md)** - Complete system demonstration with visual documentation
- **[Future Roadmap](./FUTURE_ROADMAP.md)** - Planned enhancements and architectural evolution
- **[Test Documentation](../tests/README.md)** - Test organization and execution guide
- **[Logging Configuration](../core/logging_config.md)** - Structured JSON logging setup and configuration
- **[Configuration Management](../core/config/README.md)** - Centralized configuration system using Pydantic BaseSettings
- **[Original Architecture](./original_full_system_architecture.md)** - Complete Phase 1 documentation and initial system design
- **[Project Overview](../../README.md)** - High-level project description and objectives

---

## 1. Introduction

This document provides a comprehensive overview of the system architecture for the Smart Maintenance SaaS platform. The platform is designed as a cloud-native, multi-agent system that leverages an event-driven architecture to deliver a modular, scalable, and resilient solution for predictive maintenance in the industrial sector.

### 1.1. Project Objectives

The primary goal of this project is to create a sophisticated backend system that can:

- **Ingest and Process Real-Time IoT Data:** Handle high volumes of sensor data from industrial equipment.
- **Detect and Validate Anomalies:** Use a combination of machine learning and statistical models to identify potential issues and validate them to reduce false positives.
- **Predict Failures:** Forecast potential equipment failures and estimate the time to failure (TTF).
- **Automate Maintenance Workflows:** Orchestrate the entire maintenance lifecycle, from anomaly detection to scheduling and logging completed tasks.
- **Learn and Adapt:** Continuously improve its performance by learning from system feedback and historical data.

---

## üéØ 2. System Architecture Visualizations

This section provides multiple perspectives on the Smart Maintenance SaaS architecture through comprehensive diagrams. Each diagram focuses on different aspects of the system to provide a complete understanding.

### üìä 2.1. High-Level System Overview

```mermaid
graph TB
    subgraph "External Systems"
        IOT[üè≠ IoT Sensors]
        USERS[üë• Users/Dashboard]
        MOBILE[üì± Mobile Apps]
    end

    subgraph "API Layer"
        LB[‚öñÔ∏è Load Balancer]
        API[üöÄ FastAPI Gateway]
        AUTH[üîê Authentication]
    end

    subgraph "Core Processing"
        COORD[üéØ System Coordinator]
        EVENT[üì° Event Bus]
        AGENTS[ü§ñ Multi-Agent System]
    end

    subgraph "Data Layer"
        TS[(‚è∞ TimescaleDB)]
        VEC[(üß† ChromaDB)]
        CACHE[(‚ö° Redis Cache)]
    end

    subgraph "Infrastructure"
        DOCKER[üê≥ Docker Containers]
        MONITOR[üìä Monitoring]
        LOGS[üìù Centralized Logging]
    end

    IOT --> LB
    USERS --> LB
    MOBILE --> LB
    LB --> API
    API --> AUTH
    AUTH --> COORD
    COORD --> EVENT
    EVENT --> AGENTS
    AGENTS --> TS
    AGENTS --> VEC
    AGENTS --> CACHE
    COORD --> DOCKER
    AGENTS --> MONITOR
    EVENT --> LOGS

    classDef external fill:#e1f5fe
    classDef api fill:#f3e5f5
    classDef core fill:#e8f5e8
    classDef data fill:#fff3e0
    classDef infra fill:#fce4ec

    class IOT,USERS,MOBILE external
    class LB,API,AUTH api
    class COORD,EVENT,AGENTS core
    class TS,VEC,CACHE data
    class DOCKER,MONITOR,LOGS infra
```

### üîÑ 2.2. Agent Interaction Flow Diagram

```mermaid
sequenceDiagram
    participant API as API Gateway
    participant DAA as Data Acquisition
    participant EB as Event Bus
    participant ADA as Anomaly Detection
    participant VA as Validation Agent
    participant OA as Orchestrator
    participant PA as Prediction Agent
    participant SA as Scheduling Agent
    participant NA as Notification Agent
    participant MLA as Maintenance Log

    API->>+DAA: Sensor Data
    DAA->>DAA: Validate & Enrich
    DAA->>EB: DataProcessedEvent
    EB->>+ADA: Process Data
    ADA->>ADA: ML Analysis
    ADA->>EB: AnomalyDetectedEvent
    EB->>+VA: Validate Anomaly
    VA->>VA: Apply Rules & Context
    VA->>EB: AnomalyValidatedEvent
    EB->>+OA: Orchestrate Decision
    OA->>OA: Decision Logic
    OA->>EB: TriggerPredictionEvent
    EB->>+PA: Generate Prediction
    PA->>PA: Prophet Analysis
    PA->>EB: MaintenancePredictedEvent
    EB->>+SA: Schedule Maintenance
    SA->>SA: Optimize Schedule
    SA->>EB: MaintenanceScheduledEvent
    EB->>+NA: Send Notifications
    NA->>NA: Multi-channel Notify
    EB->>+MLA: Log Maintenance
    MLA->>MLA: Record History
```

### üåä 2.3. Data Pipeline Architecture

```mermaid
flowchart LR
    subgraph "Data Ingestion"
        SENSORS[üì° IoT Sensors]
        API_IN[üîå API Endpoints]
        BATCH[üì¶ Batch Import]
    end

    subgraph "Processing Pipeline"
        VALIDATE[‚úÖ Data Validation]
        ENRICH[üîÑ Data Enrichment]
        NORMALIZE[‚öñÔ∏è Normalization]
        ANOMALY[üîç Anomaly Detection]
    end

    subgraph "Storage & Analytics"
        TIMESERIES[(‚è∞ Time Series DB)]
        VECTOR[(üß† Vector DB)]
        WAREHOUSE[(üè¢ Data Warehouse)]
        CACHE[(‚ö° Cache Layer)]
    end

    subgraph "Machine Learning"
        TRAIN[üéì Model Training]
        PREDICT[üîÆ Predictions]
        FEEDBACK[üîÑ Feedback Loop]
    end

    subgraph "Output Systems"
        DASHBOARD[üìä Dashboards]
        ALERTS[üö® Alerts]
        REPORTS[üìà Reports]
        API_OUT[üì§ API Responses]
    end

    SENSORS --> VALIDATE
    API_IN --> VALIDATE
    BATCH --> VALIDATE
    
    VALIDATE --> ENRICH
    ENRICH --> NORMALIZE
    NORMALIZE --> ANOMALY
    
    ANOMALY --> TIMESERIES
    ANOMALY --> VECTOR
    NORMALIZE --> WAREHOUSE
    ENRICH --> CACHE
    
    TIMESERIES --> TRAIN
    VECTOR --> PREDICT
    WAREHOUSE --> FEEDBACK
    
    TRAIN --> DASHBOARD
    PREDICT --> ALERTS
    FEEDBACK --> REPORTS
    CACHE --> API_OUT

    classDef ingestion fill:#e3f2fd
    classDef processing fill:#e8f5e8
    classDef storage fill:#fff3e0
    classDef ml fill:#f3e5f5
    classDef output fill:#fce4ec

    class SENSORS,API_IN,BATCH ingestion
    class VALIDATE,ENRICH,NORMALIZE,ANOMALY processing
    class TIMESERIES,VECTOR,WAREHOUSE,CACHE storage
    class TRAIN,PREDICT,FEEDBACK ml
    class DASHBOARD,ALERTS,REPORTS,API_OUT output
```

### ‚ö° 2.4. Event-Driven Architecture Flow

```mermaid
graph TD
    subgraph "Event Sources"
        DATA_IN[üìä Data Ingestion]
        USER_ACTION[üë§ User Actions]
        SYSTEM_EVENT[‚öôÔ∏è System Events]
        TIMER[‚è∞ Scheduled Tasks]
    end

    subgraph "Event Bus Core"
        ROUTER[üì° Event Router]
        QUEUE[üì¨ Event Queue]
        DISPATCH[üöÄ Event Dispatcher]
    end

    subgraph "Event Processors"
        ANOMALY_PROC[üîç Anomaly Processor]
        VALIDATION_PROC[‚úÖ Validation Processor]
        PREDICTION_PROC[üîÆ Prediction Processor]
        SCHEDULE_PROC[üìÖ Schedule Processor]
        NOTIFY_PROC[üì¢ Notification Processor]
    end

    subgraph "Event Persistence"
        EVENT_LOG[(üìú Event Log)]
        METRICS[(üìä Event Metrics)]
        AUDIT[(üîç Audit Trail)]
    end

    subgraph "Event Consumers"
        DASHBOARD_SUB[üìä Dashboard Updates]
        ALERT_SUB[üö® Alert System]
        REPORT_SUB[üìà Reporting]
        API_SUB[üîå API Responses]
    end

    DATA_IN --> ROUTER
    USER_ACTION --> ROUTER
    SYSTEM_EVENT --> ROUTER
    TIMER --> ROUTER

    ROUTER --> QUEUE
    QUEUE --> DISPATCH

    DISPATCH --> ANOMALY_PROC
    DISPATCH --> VALIDATION_PROC
    DISPATCH --> PREDICTION_PROC
    DISPATCH --> SCHEDULE_PROC
    DISPATCH --> NOTIFY_PROC

    ANOMALY_PROC --> EVENT_LOG
    VALIDATION_PROC --> METRICS
    PREDICTION_PROC --> AUDIT
    SCHEDULE_PROC --> EVENT_LOG
    NOTIFY_PROC --> METRICS

    DISPATCH --> DASHBOARD_SUB
    DISPATCH --> ALERT_SUB
    DISPATCH --> REPORT_SUB
    DISPATCH --> API_SUB

    classDef source fill:#e1f5fe
    classDef core fill:#e8f5e8
    classDef processor fill:#f3e5f5
    classDef persistence fill:#fff3e0
    classDef consumer fill:#fce4ec

    class DATA_IN,USER_ACTION,SYSTEM_EVENT,TIMER source
    class ROUTER,QUEUE,DISPATCH core
    class ANOMALY_PROC,VALIDATION_PROC,PREDICTION_PROC,SCHEDULE_PROC,NOTIFY_PROC processor
    class EVENT_LOG,METRICS,AUDIT persistence
    class DASHBOARD_SUB,ALERT_SUB,REPORT_SUB,API_SUB consumer
```

### üèóÔ∏è 2.5. Deployment Architecture

```mermaid
graph TB
    subgraph "Load Balancer Layer"
        LB[‚öñÔ∏è Load Balancer]
        SSL[üîí SSL Termination]
    end

    subgraph "Application Layer"
        subgraph "API Cluster"
            API1[üöÄ FastAPI Instance 1]
            API2[üöÄ FastAPI Instance 2]
            API3[üöÄ FastAPI Instance 3]
        end

        subgraph "Agent Cluster"
            AGENT1[ü§ñ Agent Pod 1]
            AGENT2[ü§ñ Agent Pod 2]
            AGENT3[ü§ñ Agent Pod 3]
        end

        subgraph "Worker Cluster"
            WORKER1[‚öôÔ∏è Background Worker 1]
            WORKER2[‚öôÔ∏è Background Worker 2]
        end
    end

    subgraph "Data Layer"
        subgraph "Primary Database"
            DB_MASTER[(üóÑÔ∏è PostgreSQL Master)]
            DB_REPLICA[(üìö PostgreSQL Replica)]
        end

        subgraph "Specialized Storage"
            TIMESCALE[(‚è∞ TimescaleDB)]
            VECTOR[(üß† ChromaDB)]
            REDIS[(‚ö° Redis Cluster)]
        end
    end

    subgraph "Monitoring & Observability"
        METRICS[üìä Prometheus]
        LOGS[üìù Elasticsearch]
        GRAFANA[üìà Grafana]
        JAEGER[üîç Jaeger Tracing]
    end

    subgraph "Infrastructure"
        DOCKER[üê≥ Docker Swarm]
        K8S[‚ò∏Ô∏è Kubernetes]
        STORAGE[üíæ Persistent Volumes]
    end

    LB --> SSL
    SSL --> API1
    SSL --> API2
    SSL --> API3

    API1 --> AGENT1
    API2 --> AGENT2
    API3 --> AGENT3

    AGENT1 --> WORKER1
    AGENT2 --> WORKER2

    API1 --> DB_MASTER
    API2 --> DB_REPLICA
    API3 --> DB_MASTER

    AGENT1 --> TIMESCALE
    AGENT2 --> VECTOR
    AGENT3 --> REDIS

    WORKER1 --> TIMESCALE
    WORKER2 --> VECTOR

    API1 --> METRICS
    AGENT1 --> LOGS
    WORKER1 --> GRAFANA

    DOCKER --> K8S
    K8S --> STORAGE

    classDef lb fill:#e1f5fe
    classDef app fill:#e8f5e8
    classDef data fill:#fff3e0
    classDef monitor fill:#f3e5f5
    classDef infra fill:#fce4ec

    class LB,SSL lb
    class API1,API2,API3,AGENT1,AGENT2,AGENT3,WORKER1,WORKER2 app
    class DB_MASTER,DB_REPLICA,TIMESCALE,VECTOR,REDIS data
    class METRICS,LOGS,GRAFANA,JAEGER monitor
    class DOCKER,K8S,STORAGE infra
```

### üß† 2.6. Machine Learning Pipeline

```mermaid
flowchart TB
    subgraph "Data Collection"
        SENSORS[üì° Sensor Data]
        HISTORICAL[üìö Historical Data]
        FEEDBACK[üîÑ Feedback Data]
    end

    subgraph "Feature Engineering"
        EXTRACT[üîç Feature Extraction]
        TRANSFORM[üîÑ Data Transformation]
        SELECT[‚úÖ Feature Selection]
    end

    subgraph "Model Training"
        ANOMALY_TRAIN[üéØ Anomaly Detection Training]
        PROPHET_TRAIN[üìà Prophet Model Training]
        VALIDATION_TRAIN[‚úÖ Validation Model Training]
    end

    subgraph "Model Deployment"
        ANOMALY_MODEL[üîç Isolation Forest Model]
        PROPHET_MODEL[üîÆ Prophet Predictor]
        ENSEMBLE[üé≠ Ensemble Decision]
    end

    subgraph "Real-time Inference"
        STREAM_DATA[üìä Streaming Data]
        PREPROCESS[‚öôÔ∏è Preprocessing]
        INFERENCE[üß† Model Inference]
        POSTPROCESS[üîß Postprocessing]
    end

    subgraph "Model Management"
        MONITOR[üìä Model Monitoring]
        RETRAIN[üîÑ Retraining Pipeline]
        VERSIONING[üì¶ Model Versioning]
    end

    SENSORS --> EXTRACT
    HISTORICAL --> EXTRACT
    FEEDBACK --> EXTRACT

    EXTRACT --> TRANSFORM
    TRANSFORM --> SELECT

    SELECT --> ANOMALY_TRAIN
    SELECT --> PROPHET_TRAIN
    SELECT --> VALIDATION_TRAIN

    ANOMALY_TRAIN --> ANOMALY_MODEL
    PROPHET_TRAIN --> PROPHET_MODEL
    VALIDATION_TRAIN --> ENSEMBLE

    STREAM_DATA --> PREPROCESS
    PREPROCESS --> INFERENCE
    
    ANOMALY_MODEL --> INFERENCE
    PROPHET_MODEL --> INFERENCE
    ENSEMBLE --> INFERENCE

    INFERENCE --> POSTPROCESS

    POSTPROCESS --> MONITOR
    MONITOR --> RETRAIN
    RETRAIN --> VERSIONING

    classDef collection fill:#e3f2fd
    classDef engineering fill:#e8f5e8
    classDef training fill:#fff3e0
    classDef deployment fill:#f3e5f5
    classDef inference fill:#fce4ec
    classDef management fill:#e1f5fe

    class SENSORS,HISTORICAL,FEEDBACK collection
    class EXTRACT,TRANSFORM,SELECT engineering
    class ANOMALY_TRAIN,PROPHET_TRAIN,VALIDATION_TRAIN training
    class ANOMALY_MODEL,PROPHET_MODEL,ENSEMBLE deployment
    class STREAM_DATA,PREPROCESS,INFERENCE,POSTPROCESS inference
    class MONITOR,RETRAIN,VERSIONING management
```

---

## 3. System Architecture

The architecture is designed around a multi-agent system where specialized agents perform specific tasks. These agents communicate asynchronously through an **Event Bus**, creating a decoupled and highly scalable system.

### 3.1. Core Components

#### a. API Gateway (FastAPI)

The **API Gateway**, built with FastAPI, is the primary entry point for all external interactions. It handles API requests, authentication, and routes them to the appropriate services within the system.

#### b. System Coordinator

The `SystemCoordinator` is the central nervous system of the platform. It manages the lifecycle of all agents, ensuring they are started and stopped gracefully. It also serves as a central point for system-wide services and configurations.

#### c. Event Bus

The `EventBus` is a custom, in-memory, asynchronous messaging system that enables decoupled communication between agents. It allows agents to publish events and subscribe to events they are interested in, forming the backbone of the event-driven architecture.

#### d. Multi-Agent System

This is the core of the platform, consisting of several specialized agents that work together to perform complex tasks. Each agent is designed to be autonomous and responsible for a specific part of the workflow.

#### e. Database (PostgreSQL with TimescaleDB)

A **PostgreSQL** database with the **TimescaleDB** extension is used for data persistence. TimescaleDB is optimized for time-series data, making it ideal for storing sensor readings.

### 3.2. Agent Descriptions

| Agent                       | Role and Responsibilities                                                                                                                                                                                                  |
| --------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **DataAcquisitionAgent** | Ingests raw sensor data, validates its structure and quality, enriches it with additional context, and publishes it for further processing.                                                                                   |
| **AnomalyDetectionAgent** | Subscribes to processed data and uses a dual-method approach (Isolation Forest and statistical models) to detect anomalies. It calculates a confidence score for each potential anomaly.                                         |
| **ValidationAgent** | Receives detected anomalies and validates them by applying a rule engine and analyzing historical context to reduce false positives. It adjusts the confidence score and assigns a validation status.                          |
| **OrchestratorAgent** | The central coordinator of the workflow. It listens for events from various agents and makes decisions on the next steps, such as escalating to a human or triggering automated actions like scheduling maintenance.             |
| **PredictionAgent** | Uses the Prophet machine learning library to analyze historical data for a validated anomaly and predict the Time-to-Failure (TTF). It generates maintenance recommendations based on its predictions.                               |
| **SchedulingAgent** | Takes maintenance predictions and schedules the required tasks. It uses a simplified optimization algorithm to assign technicians and find available time slots.                                                          |
| **NotificationAgent** | Sends notifications to technicians and stakeholders about scheduled maintenance and other important system events.                                                                                                        |
| **HumanInterfaceAgent** | Manages human-in-the-loop decision points. It simulates human interaction for critical decisions that require approval or input that cannot be fully automated.                                                              |
| **ReportingAgent** | Generates analytics reports, visualizations, and actionable insights related to maintenance operations, equipment health, and system performance.                                                                           |
| **LearningAgent** | Implements a Retrieval-Augmented Generation (RAG) system using ChromaDB and SentenceTransformers. It learns from system feedback and maintenance logs to provide context-aware insights and improve system accuracy over time. |
| **MaintenanceLogAgent** | Subscribes to maintenance completion events and records the details in the database, closing the maintenance workflow loop and providing a historical record of all maintenance activities.                                    |

### 3.3. System Architecture Diagram

```mermaid
graph TD
    subgraph "External Interfaces"
        UI[User Interface / API Clients]
    end

    subgraph "Backend System"
        API[API Gateway - FastAPI]
        EventBus[Event Bus]
        SystemCoordinator[System Coordinator]

        subgraph "Agents"
            DAA[Data Acquisition Agent]
            ADA[Anomaly Detection Agent]
            VA[Validation Agent]
            Orch[Orchestrator Agent]
            PA[Prediction Agent]
            SA[Scheduling Agent]
            NA[Notification Agent]
            HIA[Human Interface Agent]
            RA[Reporting Agent]
            LA[Learning Agent]
            MLA[Maintenance Log Agent]
        end

        subgraph "Data Persistence"
            DB[(TimescaleDB)]
            VDB[(ChromaDB)]
        end
    end

    UI --> API
    API --> SystemCoordinator
    SystemCoordinator -.-> DAA
    SystemCoordinator -.-> ADA
    SystemCoordinator -.-> VA
    SystemCoordinator -.-> Orch
    SystemCoordinator -.-> PA
    SystemCoordinator -.-> SA
    SystemCoordinator -.-> NA
    SystemCoordinator -.-> HIA
    SystemCoordinator -.-> RA
    SystemCoordinator -.-> LA
    SystemCoordinator -.-> MLA

    DAA --> EventBus
    EventBus --> ADA
    ADA --> EventBus
    EventBus --> VA
    VA --> EventBus
    EventBus --> Orch
    Orch --> EventBus
    EventBus --> PA
    EventBus --> HIA
    PA --> EventBus
    HIA --> EventBus
    EventBus --> SA
    SA --> EventBus
    EventBus --> NA
    EventBus --> MLA

    DAA --> DB
    VA --> DB
    PA --> DB
    MLA --> DB
    LA --> VDB
    RA --> DB
    RA --> VDB
```

### 3.4. Data Flow

1. **Ingestion:** Sensor data is sent to the API Gateway and ingested by the DataAcquisitionAgent.
2. **Processing:** The data is validated, enriched, and stored in TimescaleDB. A DataProcessedEvent is published.
3. **Anomaly Detection:** The AnomalyDetectionAgent detects potential anomalies and publishes an AnomalyDetectedEvent.
4. **Validation:** The ValidationAgent validates the anomaly and publishes an AnomalyValidatedEvent.
5. **Orchestration:** The OrchestratorAgent receives the validated anomaly and decides the next steps.
6. **Prediction:** If the anomaly is credible, the OrchestratorAgent may trigger the PredictionAgent, which forecasts the time to failure and publishes a MaintenancePredictedEvent.
7. **Scheduling:** The SchedulingAgent schedules the maintenance task and publishes a MaintenanceScheduledEvent.
8. **Notification:** The NotificationAgent sends notifications about the scheduled task.
9. **Logging:** Once the maintenance is complete, the MaintenanceLogAgent records the details in the database.
10. **Learning:** The LearningAgent continuously learns from feedback and maintenance logs to improve the system.

---

## 4. Architectural Decisions & Future Enhancements

### 4.1. Project Evolution: Plan vs. Implementation

This checklist provides a transparent breakdown of the features and technologies outlined in the initial "Hermes Backend Plan" versus what was ultimately implemented in the codebase during the 14-day sprint. The "My Opinion" column offers my rationale for the architectural trade-offs that I made.

| Component | Planned in "Hermes Backend Plan" | Implemented in Codebase | My Opinion |
|-----------|----------------------------------|-------------------------|-------------------------|
| **API & Gateway** | FastAPI, GraphQL, WebSocket Hub | FastAPI (REST API only). The API is functional with endpoints for ingestion, reporting, and decisions | **Good decision.** I chose not to implement GraphQL and WebSockets as they would require significant effort. A standard REST API is more than sufficient for our core functionality and deliverables. I'm keeping it as is. |
| **Event Streaming** | Apache Kafka, Redis Streams, Event Sourcing | Custom In-Memory `EventBus`. My `core/events/event_bus.py` is a custom, asynchronous pub/sub system | **Excellent trade-off.** This was my most significant architectural deviation, and I stand by it. A full Kafka setup would be too complex. My custom event bus achieves the required decoupling for the agents to function in an event-driven manner, which was my primary goal. |
| **Agent Workflow** | Temporal.io, LangGraph, Service Mesh | Implicit Orchestration via the `OrchestratorAgent` and direct event subscriptions between agents | **Pragmatic choice.** Like Kafka, I decided a full workflow engine like Temporal.io was unnecessary for this sprint. My `OrchestratorAgent` serves this purpose effectively for the current scope. |
| **ML: Prediction** | Prophet and LSTM for combined forecasting | Prophet only. The `PredictionAgent` is fully implemented using the Prophet library | **Sufficient and strong.** I chose Prophet as it's a powerful forecasting model on its own. Adding LSTM would increase complexity for potentially marginal gains in this timeframe. What I implemented is robust and meets our prediction goal. |
| **ML: Anomaly Detection** | Scikit-learn (IsolationForest), Statistical Models, Autoencoder, Ensemble methods | Scikit-learn (IsolationForest) and Statistical Models are fully implemented in the `AnomalyDetectionAgent` with an ensemble decision method | **Fully aligned.** I successfully implemented the core of the planned anomaly detection system. I skipped autoencoders as they're complex and not necessary for a functional prototype. |
| **ML: Learning (RAG)** | RAG with ChromaDB and MLflow for MLOps | RAG with ChromaDB and SentenceTransformers is implemented in the `LearningAgent`. MLflow is not used | **Excellent work.** I prioritized implementing the RAG portion as it's a major feature. I omitted MLflow as it's an MLOps tool for experiment tracking and not critical for our core backend functionality. |
| **Scheduling** | OR-Tools for constraint optimization | The `ortools` dependency is in `pyproject.toml`, but the `SchedulingAgent` uses a simplified "greedy" logic. The OR-Tools code is commented out | **Partially implemented.** This is the one area where my implementation is incomplete but I've laid the foundation. Given our time constraints, I used a greedy approach as a functional placeholder. |
| **Databases** | TimescaleDB, Vector DB (Chroma), Redis | TimescaleDB and ChromaDB are both used. Redis is installed but not actively used for caching or rate-limiting yet | **Excellent.** I've implemented the two most critical and novel database technologies from the plan. Redis caching is an optimization that I can add later. |



### 4.2. Machine Learning Implementation Deep Dive

Our machine learning implementation is solid and aligns well with the project's goals.

**Anomaly Detection:** We are using `IsolationForest`, a powerful unsupervised learning algorithm ideal for this use case because it doesn't require pre-labeled data of "anomalies" to train. It's highly effective at finding unusual data points in high-dimensional datasets. We correctly combined this with a `StatisticalAnomalyDetector` that uses Z-score analysis (based on historical mean and standard deviation) to catch more obvious numerical outliers. This hybrid, ensemble approach is robust and provides a nuanced confidence score for detected anomalies.

**Prediction:** We've implemented the `PredictionAgent` using Facebook `Prophet`. Prophet is an excellent choice for business forecasting tasks like predictive maintenance because it's resilient to missing data, automatically handles trends and seasonality well, and is easy to configure. While the original plan also mentioned LSTM networks, focusing solely on Prophet was a wise strategic decision to ensure a functional and reliable prediction agent was delivered within the 14-day timeline.

### 4.3. Rationale for Current Agentic Framework

**Why We Chose a Multi-Agent Architecture:**

1. **Modularity:** Each agent has a clear and well-defined responsibility, making development, testing, and maintenance easier.
2. **Scalability:** Individual agents can be scaled independently based on demand.
3. **Resilience:** If one agent fails, others can continue to operate, and the system can recover gracefully.
4. **Extensibility:** New agents can be easily added to the system without affecting the existing ones.

**Advantages of Our EventBus Implementation:**

- **Low Latency:** In-memory communication is faster than networked messaging solutions.
- **Simplicity:** Less operational complexity compared to external messaging systems.
- **Rapid Development:** Enables quick prototyping and iteration.

---

## Smart Maintenance SaaS - Sistema e Arquitetura (Portugu√™s)

## üìö Navega√ß√£o da Documenta√ß√£o

Este documento faz parte da su√≠te de documenta√ß√£o do Smart Maintenance SaaS. Para um entendimento completo do sistema, consulte tamb√©m:

- **[README do Backend](../README.md)** - Guia de implanta√ß√£o Docker e introdu√ß√£o
- **[Status de Implanta√ß√£o](./DEPLOYMENT_STATUS.md)** - Status atual de implanta√ß√£o e informa√ß√µes do container
- **[Documenta√ß√£o da API](./api.md)** - Refer√™ncia completa da API REST e exemplos de uso
- **[Baseline de Performance](./PERFORMANCE_BASELINE.md)** - Resultados de testes de carga e baseline de m√©tricas de performance
- **[Instru√ß√µes de Teste de Carga](./LOAD_TESTING_INSTRUCTIONS.md)** - Guia abrangente para executar testes de performance
- **[Capturas de Tela do Sistema](./SYSTEM_SCREENSHOTS.md)** - Demonstra√ß√£o completa do sistema com documenta√ß√£o visual
- **[Roadmap Futuro](./FUTURE_ROADMAP.md)** - Melhorias planejadas e evolu√ß√£o arquitetural
- **[Documenta√ß√£o de Testes](../tests/README.md)** - Organiza√ß√£o de testes e guia de execu√ß√£o
- **[Configura√ß√£o de Logging](../core/logging_config.md)** - Configura√ß√£o de logging JSON estruturado
- **[Gerenciamento de Configura√ß√£o](../core/config/README.md)** - Sistema de configura√ß√£o centralizado usando Pydantic BaseSettings
- **[Arquitetura Original](./original_full_system_architecture.md)** - Documenta√ß√£o completa da Fase 1 e design inicial do sistema
- **[Vis√£o Geral do Projeto](../../README.md)** - Descri√ß√£o de alto n√≠vel e objetivos do projeto

---

## 1. Introdu√ß√£o

Este documento fornece uma vis√£o geral abrangente da arquitetura de sistema para a plataforma Smart Maintenance SaaS. A plataforma foi projetada como um sistema multi-agente nativo da nuvem, que utiliza uma arquitetura orientada a eventos para fornecer uma solu√ß√£o modular, escal√°vel e resiliente para manuten√ß√£o preditiva no setor industrial.

### 1.1. Objetivos do Projeto

O objetivo principal deste projeto √© criar um sistema backend sofisticado que possa:

- **Ingerir e Processar Dados IoT em Tempo Real:** Lidar com grandes volumes de dados de sensores de equipamentos industriais.
- **Detectar e Validar Anomalias:** Usar uma combina√ß√£o de aprendizado de m√°quina e modelos estat√≠sticos para identificar problemas potenciais e valid√°-los para reduzir falsos positivos.
- **Prever Falhas:** Prever falhas potenciais de equipamentos e estimar o tempo at√© a falha (TTF).
- **Automatizar Fluxos de Trabalho de Manuten√ß√£o:** Orquestrar todo o ciclo de vida da manuten√ß√£o, desde a detec√ß√£o de anomalias at√© o agendamento e registro de tarefas conclu√≠das.
- **Aprender e Adaptar:** Melhorar continuamente seu desempenho aprendendo com o feedback do sistema e dados hist√≥ricos.

---

## üéØ 2. Visualiza√ß√µes da Arquitetura do Sistema

Esta se√ß√£o fornece m√∫ltiplas perspectivas da arquitetura do Smart Maintenance SaaS atrav√©s de diagramas abrangentes. Cada diagrama foca em diferentes aspectos do sistema para fornecer uma compreens√£o completa.

### üìä 2.1. Vis√£o Geral do Sistema de Alto N√≠vel

```mermaid
graph TB
    subgraph "Sistemas Externos"
        IOT[üè≠ Sensores IoT]
        USERS[üë• Usu√°rios/Dashboard]
        MOBILE[üì± Aplicativos M√≥veis]
    end

    subgraph "Camada API"
        LB[‚öñÔ∏è Balanceador de Carga]
        API[üöÄ Gateway FastAPI]
        AUTH[üîê Autentica√ß√£o]
    end

    subgraph "Processamento Central"
        COORD[üéØ Coordenador do Sistema]
        EVENT[üì° Barramento de Eventos]
        AGENTS[ü§ñ Sistema Multi-Agente]
    end

    subgraph "Camada de Dados"
        TS[(‚è∞ TimescaleDB)]
        VEC[(üß† ChromaDB)]
        CACHE[(‚ö° Cache Redis)]
    end

    subgraph "Infraestrutura"
        DOCKER[üê≥ Containers Docker]
        MONITOR[üìä Monitoramento]
        LOGS[üìù Logging Centralizado]
    end

    IOT --> LB
    USERS --> LB
    MOBILE --> LB
    LB --> API
    API --> AUTH
    AUTH --> COORD
    COORD --> EVENT
    EVENT --> AGENTS
    AGENTS --> TS
    AGENTS --> VEC
    AGENTS --> CACHE
    COORD --> DOCKER
    AGENTS --> MONITOR
    EVENT --> LOGS

    classDef external fill:#e1f5fe
    classDef api fill:#f3e5f5
    classDef core fill:#e8f5e8
    classDef data fill:#fff3e0
    classDef infra fill:#fce4ec

    class IOT,USERS,MOBILE external
    class LB,API,AUTH api
    class COORD,EVENT,AGENTS core
    class TS,VEC,CACHE data
    class DOCKER,MONITOR,LOGS infra
```

### üîÑ 2.2. Diagrama de Fluxo de Intera√ß√£o dos Agentes

```mermaid
sequenceDiagram
    participant API as Gateway API
    participant DAA as Aquisi√ß√£o de Dados
    participant EB as Barramento de Eventos
    participant ADA as Detec√ß√£o de Anomalias
    participant VA as Agente de Valida√ß√£o
    participant OA as Orquestrador
    participant PA as Agente de Previs√£o
    participant SA as Agente de Agendamento
    participant NA as Agente de Notifica√ß√£o
    participant MLA as Log de Manuten√ß√£o

    API->>+DAA: Dados do Sensor
    DAA->>DAA: Validar e Enriquecer
    DAA->>EB: DataProcessedEvent
    EB->>+ADA: Processar Dados
    ADA->>ADA: An√°lise ML
    ADA->>EB: AnomalyDetectedEvent
    EB->>+VA: Validar Anomalia
    VA->>VA: Aplicar Regras e Contexto
    VA->>EB: AnomalyValidatedEvent
    EB->>+OA: Orquestrar Decis√£o
    OA->>OA: L√≥gica de Decis√£o
    OA->>EB: TriggerPredictionEvent
    EB->>+PA: Gerar Previs√£o
    PA->>PA: An√°lise Prophet
    PA->>EB: MaintenancePredictedEvent
    EB->>+SA: Agendar Manuten√ß√£o
    SA->>SA: Otimizar Agenda
    SA->>EB: MaintenanceScheduledEvent
    EB->>+NA: Enviar Notifica√ß√µes
    NA->>NA: Notificar Multi-canal
    EB->>+MLA: Registrar Manuten√ß√£o
    MLA->>MLA: Gravar Hist√≥rico
```

### üåä 2.3. Arquitetura do Pipeline de Dados

```mermaid
flowchart LR
    subgraph "Ingest√£o de Dados"
        SENSORS[üì° Sensores IoT]
        API_IN[üîå Endpoints da API]
        BATCH[üì¶ Importa√ß√£o em Lote]
    end

    subgraph "Pipeline de Processamento"
        VALIDATE[‚úÖ Valida√ß√£o de Dados]
        ENRICH[üîÑ Enriquecimento de Dados]
        NORMALIZE[‚öñÔ∏è Normaliza√ß√£o]
        ANOMALY[üîç Detec√ß√£o de Anomalias]
    end

    subgraph "Armazenamento e Analytics"
        TIMESERIES[(‚è∞ BD de S√©ries Temporais)]
        VECTOR[(üß† BD Vetorial)]
        WAREHOUSE[(üè¢ Data Warehouse)]
        CACHE[(‚ö° Camada de Cache)]
    end

    subgraph "Machine Learning"
        TRAIN[üéì Treinamento de Modelo]
        PREDICT[üîÆ Previs√µes]
        FEEDBACK[üîÑ Loop de Feedback]
    end

    subgraph "Sistemas de Sa√≠da"
        DASHBOARD[üìä Dashboards]
        ALERTS[üö® Alertas]
        REPORTS[üìà Relat√≥rios]
        API_OUT[üì§ Respostas da API]
    end

    SENSORS --> VALIDATE
    API_IN --> VALIDATE
    BATCH --> VALIDATE
    
    VALIDATE --> ENRICH
    ENRICH --> NORMALIZE
    NORMALIZE --> ANOMALY
    
    ANOMALY --> TIMESERIES
    ANOMALY --> VECTOR
    NORMALIZE --> WAREHOUSE
    ENRICH --> CACHE
    
    TIMESERIES --> TRAIN
    VECTOR --> PREDICT
    WAREHOUSE --> FEEDBACK
    
    TRAIN --> DASHBOARD
    PREDICT --> ALERTS
    FEEDBACK --> REPORTS
    CACHE --> API_OUT

    classDef ingestion fill:#e3f2fd
    classDef processing fill:#e8f5e8
    classDef storage fill:#fff3e0
    classDef ml fill:#f3e5f5
    classDef output fill:#fce4ec

    class SENSORS,API_IN,BATCH ingestion
    class VALIDATE,ENRICH,NORMALIZE,ANOMALY processing
    class TIMESERIES,VECTOR,WAREHOUSE,CACHE storage
    class TRAIN,PREDICT,FEEDBACK ml
    class DASHBOARD,ALERTS,REPORTS,API_OUT output
```

### ‚ö° 2.4. Fluxo da Arquitetura Orientada a Eventos

```mermaid
graph TD
    subgraph "Fontes de Eventos"
        DATA_IN[üìä Ingest√£o de Dados]
        USER_ACTION[üë§ A√ß√µes do Usu√°rio]
        SYSTEM_EVENT[‚öôÔ∏è Eventos do Sistema]
        TIMER[‚è∞ Tarefas Agendadas]
    end

    subgraph "N√∫cleo do Barramento de Eventos"
        ROUTER[üì° Roteador de Eventos]
        QUEUE[üì¨ Fila de Eventos]
        DISPATCH[üöÄ Despachador de Eventos]
    end

    subgraph "Processadores de Eventos"
        ANOMALY_PROC[üîç Processador de Anomalias]
        VALIDATION_PROC[‚úÖ Processador de Valida√ß√£o]
        PREDICTION_PROC[üîÆ Processador de Previs√£o]
        SCHEDULE_PROC[üìÖ Processador de Agendamento]
        NOTIFY_PROC[üì¢ Processador de Notifica√ß√£o]
    end

    subgraph "Persist√™ncia de Eventos"
        EVENT_LOG[(üìú Log de Eventos)]
        METRICS[(üìä M√©tricas de Eventos)]
        AUDIT[(üîç Trilha de Auditoria)]
    end

    subgraph "Consumidores de Eventos"
        DASHBOARD_SUB[üìä Atualiza√ß√µes do Dashboard]
        ALERT_SUB[üö® Sistema de Alertas]
        REPORT_SUB[üìà Relat√≥rios]
        API_SUB[üîå Respostas da API]
    end

    DATA_IN --> ROUTER
    USER_ACTION --> ROUTER
    SYSTEM_EVENT --> ROUTER
    TIMER --> ROUTER

    ROUTER --> QUEUE
    QUEUE --> DISPATCH

    DISPATCH --> ANOMALY_PROC
    DISPATCH --> VALIDATION_PROC
    DISPATCH --> PREDICTION_PROC
    DISPATCH --> SCHEDULE_PROC
    DISPATCH --> NOTIFY_PROC

    ANOMALY_PROC --> EVENT_LOG
    VALIDATION_PROC --> METRICS
    PREDICTION_PROC --> AUDIT
    SCHEDULE_PROC --> EVENT_LOG
    NOTIFY_PROC --> METRICS

    DISPATCH --> DASHBOARD_SUB
    DISPATCH --> ALERT_SUB
    DISPATCH --> REPORT_SUB
    DISPATCH --> API_SUB

    classDef source fill:#e1f5fe
    classDef core fill:#e8f5e8
    classDef processor fill:#f3e5f5
    classDef persistence fill:#fff3e0
    classDef consumer fill:#fce4ec

    class DATA_IN,USER_ACTION,SYSTEM_EVENT,TIMER source
    class ROUTER,QUEUE,DISPATCH core
    class ANOMALY_PROC,VALIDATION_PROC,PREDICTION_PROC,SCHEDULE_PROC,NOTIFY_PROC processor
    class EVENT_LOG,METRICS,AUDIT persistence
    class DASHBOARD_SUB,ALERT_SUB,REPORT_SUB,API_SUB consumer
```

### üèóÔ∏è 2.5. Arquitetura de Implanta√ß√£o

```mermaid
graph TB
    subgraph "Camada de Balanceador de Carga"
        LB[‚öñÔ∏è Balanceador de Carga]
        SSL[üîí Termina√ß√£o SSL]
    end

    subgraph "Camada de Aplica√ß√£o"
        subgraph "Cluster API"
            API1[üöÄ Inst√¢ncia FastAPI 1]
            API2[üöÄ Inst√¢ncia FastAPI 2]
            API3[üöÄ Inst√¢ncia FastAPI 3]
        end

        subgraph "Cluster de Agentes"
            AGENT1[ü§ñ Pod de Agente 1]
            AGENT2[ü§ñ Pod de Agente 2]
            AGENT3[ü§ñ Pod de Agente 3]
        end

        subgraph "Cluster de Workers"
            WORKER1[‚öôÔ∏è Worker em Background 1]
            WORKER2[‚öôÔ∏è Worker em Background 2]
        end
    end

    subgraph "Camada de Dados"
        subgraph "Banco de Dados Prim√°rio"
            DB_MASTER[(üóÑÔ∏è PostgreSQL Master)]
            DB_REPLICA[(üìö PostgreSQL Replica)]
        end

        subgraph "Armazenamento Especializado"
            TIMESCALE[(‚è∞ TimescaleDB)]
            VECTOR[(üß† ChromaDB)]
            REDIS[(‚ö° Cluster Redis)]
        end
    end

    subgraph "Monitoramento e Observabilidade"
        METRICS[üìä Prometheus]
        LOGS[üìù Elasticsearch]
        GRAFANA[üìà Grafana]
        JAEGER[üîç Jaeger Tracing]
    end

    subgraph "Infraestrutura"
        DOCKER[üê≥ Docker Swarm]
        K8S[‚ò∏Ô∏è Kubernetes]
        STORAGE[üíæ Volumes Persistentes]
    end

    LB --> SSL
    SSL --> API1
    SSL --> API2
    SSL --> API3

    API1 --> AGENT1
    API2 --> AGENT2
    API3 --> AGENT3

    AGENT1 --> WORKER1
    AGENT2 --> WORKER2

    API1 --> DB_MASTER
    API2 --> DB_REPLICA
    API3 --> DB_MASTER

    AGENT1 --> TIMESCALE
    AGENT2 --> VECTOR
    AGENT3 --> REDIS

    WORKER1 --> TIMESCALE
    WORKER2 --> VECTOR

    API1 --> METRICS
    AGENT1 --> LOGS
    WORKER1 --> GRAFANA

    DOCKER --> K8S
    K8S --> STORAGE

    classDef lb fill:#e1f5fe
    classDef app fill:#e8f5e8
    classDef data fill:#fff3e0
    classDef monitor fill:#f3e5f5
    classDef infra fill:#fce4ec

    class LB,SSL lb
    class API1,API2,API3,AGENT1,AGENT2,AGENT3,WORKER1,WORKER2 app
    class DB_MASTER,DB_REPLICA,TIMESCALE,VECTOR,REDIS data
    class METRICS,LOGS,GRAFANA,JAEGER monitor
    class DOCKER,K8S,STORAGE infra
```

### üß† 2.6. Pipeline de Machine Learning

```mermaid
flowchart TB
    subgraph "Coleta de Dados"
        SENSORS[üì° Dados de Sensores]
        HISTORICAL[üìö Dados Hist√≥ricos]
        FEEDBACK[üîÑ Dados de Feedback]
    end

    subgraph "Engenharia de Features"
        EXTRACT[üîç Extra√ß√£o de Features]
        TRANSFORM[üîÑ Transforma√ß√£o de Dados]
        SELECT[‚úÖ Sele√ß√£o de Features]
    end

    subgraph "Treinamento de Modelos"
        ANOMALY_TRAIN[üéØ Treinamento de Detec√ß√£o de Anomalias]
        PROPHET_TRAIN[üìà Treinamento do Modelo Prophet]
        VALIDATION_TRAIN[‚úÖ Treinamento do Modelo de Valida√ß√£o]
    end

    subgraph "Implanta√ß√£o de Modelos"
        ANOMALY_MODEL[üîç Modelo Isolation Forest]
        PROPHET_MODEL[üîÆ Preditor Prophet]
        ENSEMBLE[üé≠ Decis√£o Ensemble]
    end

    subgraph "Infer√™ncia em Tempo Real"
        STREAM_DATA[üìä Dados em Streaming]
        PREPROCESS[‚öôÔ∏è Pr√©-processamento]
        INFERENCE[üß† Infer√™ncia do Modelo]
        POSTPROCESS[üîß P√≥s-processamento]
    end

    subgraph "Gerenciamento de Modelos"
        MONITOR[üìä Monitoramento de Modelos]
        RETRAIN[üîÑ Pipeline de Re-treinamento]
        VERSIONING[üì¶ Versionamento de Modelos]
    end

    SENSORS --> EXTRACT
    HISTORICAL --> EXTRACT
    FEEDBACK --> EXTRACT

    EXTRACT --> TRANSFORM
    TRANSFORM --> SELECT

    SELECT --> ANOMALY_TRAIN
    SELECT --> PROPHET_TRAIN
    SELECT --> VALIDATION_TRAIN

    ANOMALY_TRAIN --> ANOMALY_MODEL
    PROPHET_TRAIN --> PROPHET_MODEL
    VALIDATION_TRAIN --> ENSEMBLE

    STREAM_DATA --> PREPROCESS
    PREPROCESS --> INFERENCE
    
    ANOMALY_MODEL --> INFERENCE
    PROPHET_MODEL --> INFERENCE
    ENSEMBLE --> INFERENCE

    INFERENCE --> POSTPROCESS

    POSTPROCESS --> MONITOR
    MONITOR --> RETRAIN
    RETRAIN --> VERSIONING

    classDef collection fill:#e3f2fd
    classDef engineering fill:#e8f5e8
    classDef training fill:#fff3e0
    classDef deployment fill:#f3e5f5
    classDef inference fill:#fce4ec
    classDef management fill:#e1f5fe

    class SENSORS,HISTORICAL,FEEDBACK collection
    class EXTRACT,TRANSFORM,SELECT engineering
    class ANOMALY_TRAIN,PROPHET_TRAIN,VALIDATION_TRAIN training
    class ANOMALY_MODEL,PROPHET_MODEL,ENSEMBLE deployment
    class STREAM_DATA,PREPROCESS,INFERENCE,POSTPROCESS inference
    class MONITOR,RETRAIN,VERSIONING management
```

---

## 3. Arquitetura e Componentes Principais

A arquitetura √© projetada em torno de um sistema multi-agente, onde agentes especializados executam tarefas espec√≠ficas. Esses agentes se comunicam de forma ass√≠ncrona atrav√©s de um Barramento de Eventos (Event Bus), criando um sistema desacoplado e altamente escal√°vel.

### 3.1. Gateway da API (FastAPI)

O Gateway da API, constru√≠do com FastAPI, √© o ponto de entrada principal para todas as intera√ß√µes externas. Ele lida com as requisi√ß√µes da API, autentica√ß√£o e as encaminha para os servi√ßos apropriados dentro do sistema.

### 3.2. Coordenador do Sistema (SystemCoordinator)

O SystemCoordinator √© o sistema nervoso central da plataforma. Ele gerencia o ciclo de vida de todos os agentes, garantindo que sejam iniciados e parados de forma elegante. Ele tamb√©m serve como um ponto central para servi√ßos e configura√ß√µes de todo o sistema.

### 3.3. Barramento de Eventos (EventBus)

O EventBus √© um sistema de mensagens ass√≠ncrono personalizado, em mem√≥ria, que permite a comunica√ß√£o desacoplada entre os agentes. Ele permite que os agentes publiquem eventos e se inscrevam nos eventos de seu interesse, formando a espinha dorsal da arquitetura orientada a eventos.

### 3.4. Sistema Multi-Agente

Este √© o n√∫cleo da plataforma, consistindo em v√°rios agentes especializados que trabalham juntos para realizar tarefas complexas. Cada agente √© projetado para ser aut√¥nomo e respons√°vel por uma parte espec√≠fica do fluxo de trabalho.

### 3.5. Banco de Dados (PostgreSQL com TimescaleDB)

Um banco de dados PostgreSQL com a extens√£o TimescaleDB √© usado para a persist√™ncia de dados. O TimescaleDB √© otimizado para dados de s√©ries temporais, tornando-o ideal para armazenar leituras de sensores.

### 4. Descri√ß√£o dos Agentes

| Agente | Papel e Responsabilidades |
| ------ | ------------------------- |
| **DataAcquisitionAgent** | Ingesta dados brutos de sensores, valida sua estrutura e qualidade, enriquece-os com contexto adicional e os publica para processamento posterior. |
| **AnomalyDetectionAgent** | Inscreve-se para receber dados processados e utiliza uma abordagem de m√©todo duplo (Isolation Forest e modelos estat√≠sticos) para detectar anomalias. Calcula uma pontua√ß√£o de confian√ßa para cada anomalia potencial. |
| **ValidationAgent** | Recebe anomalias detectadas e as valida aplicando um motor de regras e analisando o contexto hist√≥rico para reduzir falsos positivos. Ajusta a pontua√ß√£o de confian√ßa e atribui um status de valida√ß√£o. |
| **OrchestratorAgent** | O coordenador central do fluxo de trabalho. Ouve eventos de v√°rios agentes e toma decis√µes sobre os pr√≥ximos passos, como escalar para um humano ou acionar a√ß√µes automatizadas, como o agendamento de manuten√ß√£o. |
| **PredictionAgent** | Utiliza a biblioteca de aprendizado de m√°quina Prophet para analisar dados hist√≥ricos de uma anomalia validada e prever o Tempo At√© a Falha (TTF). Gera recomenda√ß√µes de manuten√ß√£o com base em suas previs√µes. |
| **SchedulingAgent** | Pega as previs√µes de manuten√ß√£o e agenda as tarefas necess√°rias. Utiliza um algoritmo de otimiza√ß√£o simplificado para atribuir t√©cnicos e encontrar hor√°rios dispon√≠veis. |
| **NotificationAgent** | Envia notifica√ß√µes para t√©cnicos e partes interessadas sobre manuten√ß√µes agendadas e outros eventos importantes do sistema. |
| **HumanInterfaceAgent** | Gerencia os pontos de decis√£o humano-no-ciclo. Simula a intera√ß√£o humana para decis√µes cr√≠ticas que requerem aprova√ß√£o ou entrada que n√£o pode ser totalmente automatizada. |
| **ReportingAgent** | Gera relat√≥rios anal√≠ticos, visualiza√ß√µes e insights acion√°veis relacionados √†s opera√ß√µes de manuten√ß√£o, sa√∫de do equipamento e desempenho do sistema. |
| **LearningAgent** | Implementa um sistema de Gera√ß√£o Aumentada por Recupera√ß√£o (RAG) usando ChromaDB e SentenceTransformers. Aprende com o feedback do sistema e os registros de manuten√ß√£o para fornecer insights com reconhecimento de contexto e melhorar a precis√£o do sistema ao longo do tempo. |
| **MaintenanceLogAgent** | Inscreve-se em eventos de conclus√£o de manuten√ß√£o e registra os detalhes no banco de dados, fechando o ciclo do fluxo de trabalho de manuten√ß√£o e fornecendo um registro hist√≥rico de todas as atividades de manuten√ß√£o. |

### 5. Diagrama da Arquitetura do Sistema

```mermaid
graph TD
    subgraph "Interfaces Externas"
        UI[Interface do Usu√°rio / Clientes da API]
    end

    subgraph "Sistema Backend"
        API[Gateway da API - FastAPI]
        EventBus[Barramento de Eventos]
        SystemCoordinator[Coordenador do Sistema]

        subgraph "Agentes"
            DAA[Agente de Aquisi√ß√£o de Dados]
            ADA[Agente de Detec√ß√£o de Anomalias]
            VA[Agente de Valida√ß√£o]
            Orch[Agente Orquestrador]
            PA[Agente de Previs√£o]
            SA[Agente de Agendamento]
            NA[Agente de Notifica√ß√£o]
            HIA[Agente de Interface Humana]
            RA[Agente de Relat√≥rios]
            LA[Agente de Aprendizado]
            MLA[Agente de Log de Manuten√ß√£o]
        end

        subgraph "Persist√™ncia de Dados"
            DB[(TimescaleDB)]
            VDB[(ChromaDB)]
        end
    end

    UI --> API
    API --> SystemCoordinator
    SystemCoordinator -.-> DAA
    SystemCoordinator -.-> ADA
    SystemCoordinator -.-> VA
    SystemCoordinator -.-> Orch
    SystemCoordinator -.-> PA
    SystemCoordinator -.-> SA
    SystemCoordinator -.-> NA
    SystemCoordinator -.-> HIA
    SystemCoordinator -.-> RA
    SystemCoordinator -.-> LA
    SystemCoordinator -.-> MLA

    DAA --> EventBus
    EventBus --> ADA
    ADA --> EventBus
    EventBus --> VA
    VA --> EventBus
    EventBus --> Orch
    Orch --> EventBus
    EventBus --> PA
    EventBus --> HIA
    PA --> EventBus
    HIA --> EventBus
    EventBus --> SA
    SA --> EventBus
    EventBus --> NA
    EventBus --> MLA

    DAA --> DB
    VA --> DB
    PA --> DB
    MLA --> DB
    LA --> VDB
    RA --> DB
    RA --> VDB
```

### 6. Fluxo de Dados

1. **Ingest√£o:** Os dados do sensor s√£o enviados para o Gateway da API e ingeridos pelo DataAcquisitionAgent.
2. **Processamento:** Os dados s√£o validados, enriquecidos e armazenados no TimescaleDB. Um evento DataProcessedEvent √© publicado.
3. **Detec√ß√£o de Anomalias:** O AnomalyDetectionAgent detecta anomalias potenciais e publica um AnomalyDetectedEvent.
4. **Valida√ß√£o:** O ValidationAgent valida a anomalia e publica um AnomalyValidatedEvent.
5. **Orquestra√ß√£o:** O OrchestratorAgent recebe a anomalia validada e decide os pr√≥ximos passos.
6. **Previs√£o:** Se a anomalia √© cred√≠vel, o OrchestratorAgent pode acionar o PredictionAgent, que prev√™ o tempo at√© a falha e publica um MaintenancePredictedEvent.
7. **Agendamento:** O SchedulingAgent agenda a tarefa de manuten√ß√£o e publica um MaintenanceScheduledEvent.
8. **Notifica√ß√£o:** O NotificationAgent envia notifica√ß√µes sobre a tarefa agendada.
9. **Registro:** Uma vez que a manuten√ß√£o √© conclu√≠da, o MaintenanceLogAgent registra os detalhes no banco de dados.
10. **Aprendizado:** O LearningAgent aprende continuamente com o feedback e os registros de manuten√ß√£o para melhorar o sistema.

---

## 7. Decis√µes Arquiteturais e Melhorias Futuras (Portugu√™s)

### 7.1. Evolu√ß√£o do Projeto: Plano vs. Implementa√ß√£o

Esta lista de verifica√ß√£o fornece uma an√°lise transparente das funcionalidades e tecnologias delineadas no "Plano Backend Hermes" inicial versus o que foi efetivamente implementado no c√≥digo durante o sprint de 14 dias. A coluna "Minha Opini√£o" oferece minha justificativa para as decis√µes arquiteturais que tomei.

| Componente | Planejado no "Plano Backend Hermes" | Implementado no C√≥digo | Minha Opini√£o |
| :--- | :--- | :--- | :--- |
| **API & Gateway** | FastAPI, GraphQL, Hub WebSocket. | FastAPI (apenas REST API). A API √© funcional com endpoints para ingest√£o, relat√≥rios e decis√µes. | **Boa decis√£o.** Optei por n√£o implementar GraphQL e WebSockets pois seria um esfor√ßo significativo. Uma API REST padr√£o √© mais que suficiente para nossa funcionalidade principal e entreg√°veis. Vou manter assim. |
| **Event Streaming** | Apache Kafka, Redis Streams, Event Sourcing. | `EventBus` customizado em mem√≥ria. Meu `core/events/event_bus.py` √© um sistema pub/sub ass√≠ncrono personalizado. | **Excelente trade-off.** Este foi meu desvio arquitetural mais significativo, e tenho certeza que foi a escolha certa. Uma configura√ß√£o completa do Kafka seria muito complexa. Meu event bus personalizado alcan√ßa o desacoplamento necess√°rio para os agentes funcionarem de maneira orientada a eventos, que era meu objetivo principal. |
| **Agent Workflow** | Temporal.io, LangGraph, Service Mesh. | Orquestra√ß√£o impl√≠cita via `OrchestratorAgent` e assinaturas diretas de eventos entre agentes. | **Escolha pragm√°tica.** Como o Kafka, decidi que um motor de workflow completo como Temporal.io seria desnecess√°rio para este sprint. Meu `OrchestratorAgent` serve efetivamente a este prop√≥sito para o escopo atual. |
| **ML: Previs√£o** | Prophet e LSTM para previs√£o combinada. | Prophet apenas. O `PredictionAgent` est√° totalmente implementado usando a biblioteca Prophet. | **Suficiente e forte.** Escolhi Prophet pois √© um modelo de previs√£o poderoso por si s√≥. Adicionar LSTM aumentaria a complexidade para ganhos potencialmente marginais neste prazo. O que implementei √© robusto e atende ao objetivo de predi√ß√£o. |
| **ML: Detec√ß√£o de Anomalias** | Scikit-learn (IsolationForest), Modelos Estat√≠sticos, Autoencoder, m√©todos Ensemble. | Scikit-learn (IsolationForest) e Modelos Estat√≠sticos est√£o totalmente implementados no `AnomalyDetectionAgent` com um m√©todo de decis√£o ensemble. | **Totalmente alinhado.** Implementei com sucesso o n√∫cleo do sistema de detec√ß√£o de anomalias planejado. Deixei de fora os autoencoders pois s√£o complexos e n√£o necess√°rios para um prot√≥tipo funcional. |
| **ML: Aprendizado (RAG)** | RAG com ChromaDB e MLflow para MLOps. | RAG com ChromaDB e SentenceTransformers est√° implementado no `LearningAgent`. MLflow n√£o √© usado. | **Excelente trabalho.** Priorizei implementar a parte RAG pois √© uma funcionalidade importante. Omiti o MLflow pois √© uma ferramenta MLOps para rastreamento de experimentos e n√£o √© cr√≠tica para a funcionalidade principal do backend. |
| **Agendamento** | OR-Tools para otimiza√ß√£o com restri√ß√µes. | A depend√™ncia `ortools` est√° no `pyproject.toml`, mas o `SchedulingAgent` usa uma l√≥gica "greedy" simplificada. O c√≥digo OR-Tools est√° comentado. | **Parcialmente implementado.** Esta √© a √∫nica √°rea onde minha implementa√ß√£o est√° incompleta, mas estabeleci a base. Dados os constrangimentos de tempo, usei uma abordagem greedy como um placeholder funcional. |
| **Bancos de Dados** | TimescaleDB, Vector DB (Chroma), Redis. | TimescaleDB e ChromaDB s√£o ambos usados. Redis est√° instalado mas n√£o usado ativamente para cache ou rate-limiting ainda. | **Excelente.** Implementei as duas tecnologias de banco de dados mais cr√≠ticas e inovadoras do plano. O cache Redis √© uma otimiza√ß√£o que posso adicionar depois. |

### 7.2. Aprofundamento na Implementa√ß√£o de Machine Learning


Nossa implementa√ß√£o de machine learning √© s√≥lida e se alinha bem com os objetivos do projeto.

**Detec√ß√£o de Anomalias:** Estamos usando `IsolationForest`, um algoritmo de aprendizado n√£o supervisionado poderoso, ideal para este caso de uso porque n√£o requer dados pr√©-rotulados de "anomalias" para treinar. √â altamente eficaz em encontrar pontos de dados incomuns em conjuntos de dados de alta dimensionalidade. Combinamos corretamente isso com um `StatisticalAnomalyDetector` que usa an√°lise Z-score (baseada na m√©dia hist√≥rica e desvio padr√£o) para capturar outliers num√©ricos mais √≥bvios. Esta abordagem h√≠brida, ensemble, √© robusta e fornece uma pontua√ß√£o de confian√ßa nuan√ßada para anomalias detectadas.

**Previs√£o:** Implementamos o `PredictionAgent` usando o `Prophet` do Facebook. Prophet √© uma excelente escolha para tarefas de previs√£o empresarial como manuten√ß√£o preditiva porque √© resiliente a dados faltantes, lida automaticamente bem com tend√™ncias e sazonalidade, e √© f√°cil de configurar. Embora o plano original tamb√©m mencionasse redes LSTM, focar apenas no Prophet foi uma decis√£o estrat√©gica s√°bia para garantir que um agente de previs√£o funcional e confi√°vel fosse entregue dentro do prazo de 14 dias.

### 7.3. Justificativa para o Framework Ag√™ntico Atual

**Por que Escolhemos uma Arquitetura Multi-Agente:**

1. **Modularidade:** Cada agente tem uma responsabilidade clara e bem definida, facilitando desenvolvimento, teste e manuten√ß√£o.
2. **Escalabilidade:** Agentes individuais podem ser escalados independentemente com base na demanda.
3. **Resili√™ncia:** Se um agente falhar, outros podem continuar operando, e o sistema pode se recuperar graciosamente.
4. **Extensibilidade:** Novos agentes podem ser facilmente adicionados ao sistema sem afetar os existentes.

**Vantagens da Nossa Implementa√ß√£o EventBus:**

- **Baixa Lat√™ncia:** Comunica√ß√£o em mem√≥ria √© mais r√°pida que solu√ß√µes de rede.
- **Simplicidade:** Menos complexidade operacional comparado a sistemas de mensageria externos.
- **Desenvolvimento R√°pido:** Permite prototipagem e itera√ß√£o r√°pidas.

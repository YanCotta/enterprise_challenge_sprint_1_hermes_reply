# 30-Day Sprint Changelog

This document records all changes made during the final 30-day sprint toward delivery.

## 2025-08-11 (Days 1–3)

- Compose: Updated `smart-maintenance-saas/docker-compose.yml`
  - Services: `api` (FastAPI), `ui` (Streamlit), `db` (TimescaleDB pg15).
  - API hardening: run `alembic upgrade head` before `uvicorn`; `restart: unless-stopped`; healthcheck `GET /health` every 30s.
  - Environment: `env_file: .env` plus `DATABASE_URL` (service-to-service host `db`), `PYTHONPATH=/app`, `DISABLE_CHROMADB=true`.
  - Volumes: mount `./logs` into container for persisted JSON logs.
  - UI: points to API via internal URL `http://api:8000`; healthcheck `GET /` on 8501; depends on API+DB health.
  - DB: `timescale/timescaledb:latest-pg15`, init script mounts `infrastructure/docker/init-scripts` to enable extension.

- Alembic migration (Timescale policies): Added `alembic_migrations/versions/20250811_120000_add_timescale_policies.py`
  - Idempotently ensures `CREATE EXTENSION IF NOT EXISTS timescaledb;`.
  - Policies on `sensor_readings` hypertable:
    - Retention: `SELECT add_retention_policy('sensor_readings', INTERVAL '180 days');`
    - Compression: `ALTER TABLE sensor_readings SET (timescaledb.compress);`
    - Compression policy: `SELECT add_compression_policy('sensor_readings', INTERVAL '7 days');`
  - Optional commented CAGG definition for 1‑minute rollups (kept off for now to reduce overhead).

- DB docs & ERD artifacts
  - ERD source: `smart-maintenance-saas/docs/db/erd.dbml` with four core entities and FK from `maintenance_logs.task_id → maintenance_tasks.id`.
  - DB README: `smart-maintenance-saas/docs/db/README.md` with entities, constraints, indexes, and Timescale policies rationale.
  - Scripts:
    - `scripts/export_schema.sh`: exports schema-only SQL via `pg_dump` to `docs/db/schema.sql` (auto-downgrades async URL for pg_dump).
    - `scripts/generate_erd.sh`: optional ERD PNG export using `eralchemy2` (requires Python+Graphviz on the host).

- Schema SQL
  - `docs/db/schema.sql` generated by running `./scripts/export_schema.sh` after the stack is up.
  - Notes: Timescale warnings about circular FKs may appear during dump; schema export still completes successfully.

## Verification performed

- Brought up stack with `docker compose up -d --build`.
- Health checks:
  - API: 200 OK on `/health` and DB connectivity verified on `/health/db`.
- Confirmed Alembic runs on API start with no errors; DB extension enabled via init script.

## 2025-08-11 (Day 4)

- Ingestion hardening:
  - Endpoint: `POST /api/v1/data/ingest` now supports `Idempotency-Key` header. In-memory TTL store (10 min) prevents duplicate event publication for the same key.
  - Structure: simple dict key → (event_id, expire_ts) with periodic cleanup; safe under a single API replica.
  - Verified behavior by issuing two identical POSTs with same `Idempotency-Key`; second response returned `"status":"duplicate_ignored"` with the original `event_id`.
- Correlation/Request IDs:
  - Added `apps/api/middleware/request_id.py`. If `X-Request-ID` is present, it’s reused; otherwise a UUIDv4 is generated.
  - Middleware sets `request.state.correlation_id` for downstream use and adds `X-Request-ID` to every response.
- Scripts portability:
  - Switched scripts to `bash` shebang and marked executable.
- ERD PNG:
  - ERD PNG generation via `eralchemy2` requires Graphviz toolchain (and build tools). Since our runtime image is slim by design, prefer manual PNG export from `docs/db/erd.dbml` using a modeling tool when needed.

## Risk review and mitigations (Days 1–4)

- Compose and migrations: Low risk. Alembic upgrade runs before serving to avoid schema drift. Health checks protect dependent services.
- Timescale policies: Low operational risk. Retention/compression choices are conservative (180d retain, compress ≥7d). Can be tuned via a new migration.
- ERD generation: Toolchain heavy; intentionally not in runtime image to avoid bloat. Keep DBML as source of truth; PNG generated externally on demand.
- Idempotency cache: In-memory per replica. For multi-replica/higher durability, swap to Redis with TTL. TTL and periodic cleanup cap memory growth.
- Request IDs: Propagate for client traceability now. For full structured logs with correlation IDs, wire `logging` extras or adopt a request-context logger in a later observability task.

